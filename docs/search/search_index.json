{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Compression of Voxel Models Abstract This website contains the documentation of my research project at TU Dresden. The goal of the project is to develop an efficient algorithm for compressing voxel models. As an input format, an unsorted list of 3D integer coordinates and attribute data is used. Multiple methods for encoding geometry data including Cuboid Extraction (CE), Sparse Voxel Octrees (SVOs) with Space-Filling Curves, and Run-Length Encoding (RLE) are explained and then compared in terms of complexity, compression ratio, and real life performance. CE fails based on its high complexity, SVOs and RLE perform almost identically in terms of compression ratio and complexity. SVOs are the clear winner because of their lower real life memory requirements when reading the input data, resulting in better performance. The Free Lossless Voxel Compression (FLVC) codec is developed based on these findings. Its advantages include arbitrary attribute information per voxel, high performance, streamability, and very low memory requirements for decoding. The codec is made available through a FOSS command-line utility which can convert between various common voxel model formats and the new FLVC format. Relevant Links FLVC Command-Line Interface This documentation on GitHub Notes Warning You can browse this offline by hosting it as a static web page on a local server. Many functions such as searching or syntax highlighting won't work when opening this project with a file:// scheme. To name an example, use python3 -m http.server 8000 in the root directory to host this page locally on port 8000.","title":"Compression of Voxel Models"},{"location":"index.html#compression-of-voxel-models","text":"","title":"Compression of Voxel Models"},{"location":"index.html#abstract","text":"This website contains the documentation of my research project at TU Dresden. The goal of the project is to develop an efficient algorithm for compressing voxel models. As an input format, an unsorted list of 3D integer coordinates and attribute data is used. Multiple methods for encoding geometry data including Cuboid Extraction (CE), Sparse Voxel Octrees (SVOs) with Space-Filling Curves, and Run-Length Encoding (RLE) are explained and then compared in terms of complexity, compression ratio, and real life performance. CE fails based on its high complexity, SVOs and RLE perform almost identically in terms of compression ratio and complexity. SVOs are the clear winner because of their lower real life memory requirements when reading the input data, resulting in better performance. The Free Lossless Voxel Compression (FLVC) codec is developed based on these findings. Its advantages include arbitrary attribute information per voxel, high performance, streamability, and very low memory requirements for decoding. The codec is made available through a FOSS command-line utility which can convert between various common voxel model formats and the new FLVC format.","title":"Abstract"},{"location":"attribute_compression.html","text":"SVO Attribute Compression Geometry is the most important attribute, as it gives the model shape. A model can exist without color attributes but not without a shape. However, geometry does not inherently make up the majority of a model's information in bytes. Whether a voxel exists or not is only single-bit information, whereas color is often 24-bit information. Also consider alpha channels, weights, and many other attributes and it becomes plain to see that other attributes comprise the vast majority of information. Requirements We have already decided to use SVOs as the form of geometry compression. It is essential that whatever method we design, it must work well with an existing SVO structure. In total, we have the following requirements: good SVO interoperability low memory/time complexity when encoding/decoding models streamability exploiting redundancies in typical models high compression ratio acceptable real world performance Inspiration: Image-Based Approach Storing a 24-bit RGB color for every voxel would be very expensive. What some formats such as PNG instead do is store a delta between neighboring colors. Instead of storing deltas we could also store whether neighboring colors are exactly identical, but such a naive approach would become very ineffective when introducing small amounts of noise into the colors. Primer on PNG Filtering c|b -+- a|x x is the color to encode and a, b, c are its neighbors. In the Filtering Section of the PNG specification we can see that the colors are converted to a delta-based representation relative to their neighbors. This step however does not reduce the size of the data as \"both the inputs and outputs fit into a byte\". It is merely a pre-processing step which increases redundancy in the image bytes and thus improves DEFLATE compression, the next step . The data will be turned into mostly low values, often times exactly zero, assuming that the deltas from the prediction are low in magnitude. Applying The Approach to Three Dimensions The PNG-style approach is already very promising but needs to be heavily modified so that we can apply it to 3D models. It meets all requirements in terms of performance and streamability. An added bonus is that the entropy coder can be treated as a black box and easily swapped out by a different algorithm. We are not in any way restricted to using zlib specifically. However, it is not obvious how to extend this approach to work well with an SVO. The greatest problems arise during the decoding process: We might not always have a neighbor, unlike in an image where we only need to store the previous and the current line of pixels. Voxels can be surrounded by vast quantities of empty space. Making it possible to access already read neighbors randomly would require reading the entire SVO into memory when decoding. Random access to neighbors in an SVO would have logarithmic complexity. In an image, it would be constant. We don't make any use of the hierarchical structure of the SVO when referencing direct neighbors. There is a simple solution that solves every single one of these problems: Referencing parent nodes and encoding deltas to parents instead of deltas to direct neighbors. Since every node has a parent (except for the root node) and parents are very easy to access during encoding and decoding, we resolve almost all problems regarding space/time/memory requirements. The center of parent nodes is also located half a voxel away from the center of its child nodes, which is why their attributes are typically similar. In an SVO which encodes voxel models, only the leaf nodes have any attribute values. The leaf nodes represent the voxels, all layers above are just subdivisions of space. Before we can compute a delta from the parents, we must propagate the values of children upwards in the tree in a mipmapping step. For example, the parent could be assigned the minimum, maximum, average, etc. of all its children. See the FLVC Data Stream for how this was implemented in practice. This mipmapping step does increase the size of the model initially, but if the values of the children are all very similar, there will be a net benefit for the entropy coder.","title":"SVO Attribute Compression"},{"location":"attribute_compression.html#svo-attribute-compression","text":"Geometry is the most important attribute, as it gives the model shape. A model can exist without color attributes but not without a shape. However, geometry does not inherently make up the majority of a model's information in bytes. Whether a voxel exists or not is only single-bit information, whereas color is often 24-bit information. Also consider alpha channels, weights, and many other attributes and it becomes plain to see that other attributes comprise the vast majority of information.","title":"SVO Attribute Compression"},{"location":"attribute_compression.html#requirements","text":"We have already decided to use SVOs as the form of geometry compression. It is essential that whatever method we design, it must work well with an existing SVO structure. In total, we have the following requirements: good SVO interoperability low memory/time complexity when encoding/decoding models streamability exploiting redundancies in typical models high compression ratio acceptable real world performance","title":"Requirements"},{"location":"attribute_compression.html#inspiration-image-based-approach","text":"Storing a 24-bit RGB color for every voxel would be very expensive. What some formats such as PNG instead do is store a delta between neighboring colors. Instead of storing deltas we could also store whether neighboring colors are exactly identical, but such a naive approach would become very ineffective when introducing small amounts of noise into the colors.","title":"Inspiration: Image-Based Approach"},{"location":"attribute_compression.html#primer-on-png-filtering","text":"c|b -+- a|x x is the color to encode and a, b, c are its neighbors. In the Filtering Section of the PNG specification we can see that the colors are converted to a delta-based representation relative to their neighbors. This step however does not reduce the size of the data as \"both the inputs and outputs fit into a byte\". It is merely a pre-processing step which increases redundancy in the image bytes and thus improves DEFLATE compression, the next step . The data will be turned into mostly low values, often times exactly zero, assuming that the deltas from the prediction are low in magnitude.","title":"Primer on PNG Filtering"},{"location":"attribute_compression.html#applying-the-approach-to-three-dimensions","text":"The PNG-style approach is already very promising but needs to be heavily modified so that we can apply it to 3D models. It meets all requirements in terms of performance and streamability. An added bonus is that the entropy coder can be treated as a black box and easily swapped out by a different algorithm. We are not in any way restricted to using zlib specifically. However, it is not obvious how to extend this approach to work well with an SVO. The greatest problems arise during the decoding process: We might not always have a neighbor, unlike in an image where we only need to store the previous and the current line of pixels. Voxels can be surrounded by vast quantities of empty space. Making it possible to access already read neighbors randomly would require reading the entire SVO into memory when decoding. Random access to neighbors in an SVO would have logarithmic complexity. In an image, it would be constant. We don't make any use of the hierarchical structure of the SVO when referencing direct neighbors. There is a simple solution that solves every single one of these problems: Referencing parent nodes and encoding deltas to parents instead of deltas to direct neighbors. Since every node has a parent (except for the root node) and parents are very easy to access during encoding and decoding, we resolve almost all problems regarding space/time/memory requirements. The center of parent nodes is also located half a voxel away from the center of its child nodes, which is why their attributes are typically similar. In an SVO which encodes voxel models, only the leaf nodes have any attribute values. The leaf nodes represent the voxels, all layers above are just subdivisions of space. Before we can compute a delta from the parents, we must propagate the values of children upwards in the tree in a mipmapping step. For example, the parent could be assigned the minimum, maximum, average, etc. of all its children. See the FLVC Data Stream for how this was implemented in practice. This mipmapping step does increase the size of the model initially, but if the values of the children are all very similar, there will be a net benefit for the entropy coder.","title":"Applying The Approach to Three Dimensions"},{"location":"cuboid_extraction.html","text":"Cuboid Extraction Motivation Voxel models of certain types have large volumes of voxels. These types include extruded heightmaps, medical scans, video game worlds and voxel art. Other types such as voxelized polygonal models may also be filled up instead of being kept hollow. The volumes may often be misaligned with the octree structure. Consider a single 2x2x2 cube in the center of an octree. All 8 subtrees of the octree would have to encode an individual voxel of this cube, which is far from optimal. In addition, decoding volumes which are completely filled simplifies the decoding process. These volumes could be extracted and encoded as cuboids like {triple<u8> pos, triple<u8> size} within a container that has dimensions up to 256^3. However we extract cuboids, those should be as large in volume as possible. This way, we cover the most voxels with our pairs of positions. The highest-volume container should be found, then extracted and the next-highest-volume container found, etc. We invest 6 bytes into the extraction, so our volume needs to be 48 to break even (e.g. 4x4x3 ). Absolutely Stupid Method extreme complexity but optimal results Since a cuboid is just a pair of positions, we can simply iterate over all pairs of positions. We then iterate over all voxels in the cuboid to test whether they exist. For a d*d*d model containing v voxels the complexity is thus O(d^3 * d^3 * d^3) or O(d^9) or O(v^3) . This is some clown-world-tier complexity and should not find its way into any serious implementation. Due to its simplicity, it could be used to verify the correctness of more complex approaches though. XYZ-Merge best possible complexity not optimal An XYZ-merge works by merging all neighboring voxels into lines on one axis first. Then neighboring lines are merged into planes on the second axis. Then neighboring planes are merged into cuboids on the last axis. The results are not optimal and may differ depending on the order of axes. At worst, we can't merge anything and thus iterate over every voxel in each of the steps once. The complexity is O(d^3 * 3) = O(d^3) or O(v) . Heightmap-Based Approaches All following approaches are based on the idea, that we can simplify this problem to a 2-dimensional one. One axis is being used as a height-axis, ideally the smallest one if the model is not cubical. We then use a sweeping-plane approach where at each coordinate, we draw a heightmap which extends into our chosen direction. Finding the largest volume in the model is reduced to finding the maximum of the largest volumes for all heightmaps. Constructing Heightmaps from Voxel-Models Here is an illustration of how this is done, in 2 dimensions, where \"up\" in text is the height axis: 0123456789 0123456789 7 ### -> 1110000000 6 #### ## # -> 2221011010 5 ## ### # # -> 3302120101 4 ##### # ## -> 4413201012 3 ## # # # -> 5504010100 2 ######## -> 6612121200 1 ######### -> 7726232310 0 ## ## #### -> 8807303420 In this illustration, # would be encoded as 1 and would be encoded as 0 Each slice from 0 to 7 is one histogram in 2 dimensions or a heightmap in 3 dimensions. We can construct these very efficiently by just taking the heightmap at one height above and incrementing each height, unless we find an empty voxel ( 0 ). Constructing Bitmaps from Heightmaps Finding the largest cuboid in a voxel-model has been reduced to finding the largest cuboid in all its heightmaps. The problem is once again reduced to finding the largest area of 1 s in a bitmap, which can be done in O(n*n) . This is done by looking at all different height-values of the heightmap. At every height, a bitmap is constructed. In our bitmap of equal dimensions, 1 represents that the height at the location is >= the height. Example: 389200 111000 323350 ------> 101110 500020 (h=3) 100000 211141 000010 Finding the largest area of 1 in an n*n rectangle has O(n^2) complexity or O(1) per pixel, when using a sweeping-line algorithm and searching for the greatest area in the resulting histograms: https://www.geeksforgeeks.org/maximum-size-rectangle-binary-sub-matrix-1s/ The volume of the greatest cuboid is then the current height multiplied with the size of the greatest area of 1 which we find. Expected values To understand the following approaches, one must realize that the highest cuboid is not necessarily the largest. For example, a very flat pyramid may have a shape which is only great in volume towards the base. For an n*n heightmap with n height, we can find the cuboid with the largest volume for all heights by finding the largest cuboid at each individual height. This forces us to check every single height. Naive Method high complexity optimal results The naive method simply to iterate over all heights. We then require O(d^2 * d^2) complexity where one d^2 represents the size of the base and the other d^2 represents the amount of pairs of heights. This would be O(v^(4/3)) , which is not as bad as the stupid approach but still polynomial. Modified Kadane's-Algorithm THIS SECTION IS WIP - complexity yet unknown - hopefully optimal results This can be done by using a sweeping-plane algorithm, where each plane is a heightmap which encodes how many voxels extrude upwards, starting from each one of its pixels. The problem is thus reduced to finding the greatest cuboid on a heightmap, which can be done somewhat efficiently using Kadane's algorithm, where any `0` is treated as negative infinity and the heights are cut-off at some maximum height dynamically during the search. For a `n*n` map, Kadane's algorithm has a complexity of `O(n^3)` or `O(n)` per pixel. Whether this is usable has to be investigated further. Golden-Section-Search (not sure if this actually works) best known complexity optimal We can achieve logarithmic complexity per voxel. The approach to this is very similar to the naive method, but based on a crucial observation. The maximum volumes at each minimum height form a unimodal function. Our search is still based on finding the greatest are of 1 in a bitmap, but we choose our heights in a smarter way instead of iterating over all heights. Consider a histogram such as: ^ 7| # 6|# # # 5|# # ### # # 4|# # ### ## ### ## # 3|# ## ### ### ### ####### ## 2|#### ### ### #### ### ####### ### 1|##### #### ### ##### ############### #### -+----------------------------------------------> |0123456789abcdefghijklmnopqrstuvwxyzABCDEFGH The greatest areas for each height are: 7: 07 (3:1, 3:7) 6: 07 (3:1, 3:7) found by extruding (3:1, 3:6) >>5: 15 (8:1, a:5) >>4: 15 (8:1, a:5) >>>>3: 21 (w:1, C:3) >>>>2: 21 (w:1, C:3) >>1: 15 (o:1, C:1) Note that the maximum volumes at each height only form a unimodal function if we extrude our found area as far up as possible. Searching for the maximum or minimum of a unimodal function can be done with logarithmic complexity. The final complexity would thus be O(n^2 * log_2 n) or O(log_2 n) per pixel.","title":"Cuboid Extraction"},{"location":"cuboid_extraction.html#cuboid-extraction","text":"","title":"Cuboid Extraction"},{"location":"cuboid_extraction.html#motivation","text":"Voxel models of certain types have large volumes of voxels. These types include extruded heightmaps, medical scans, video game worlds and voxel art. Other types such as voxelized polygonal models may also be filled up instead of being kept hollow. The volumes may often be misaligned with the octree structure. Consider a single 2x2x2 cube in the center of an octree. All 8 subtrees of the octree would have to encode an individual voxel of this cube, which is far from optimal. In addition, decoding volumes which are completely filled simplifies the decoding process. These volumes could be extracted and encoded as cuboids like {triple<u8> pos, triple<u8> size} within a container that has dimensions up to 256^3. However we extract cuboids, those should be as large in volume as possible. This way, we cover the most voxels with our pairs of positions. The highest-volume container should be found, then extracted and the next-highest-volume container found, etc. We invest 6 bytes into the extraction, so our volume needs to be 48 to break even (e.g. 4x4x3 ).","title":"Motivation"},{"location":"cuboid_extraction.html#absolutely-stupid-method","text":"extreme complexity but optimal results Since a cuboid is just a pair of positions, we can simply iterate over all pairs of positions. We then iterate over all voxels in the cuboid to test whether they exist. For a d*d*d model containing v voxels the complexity is thus O(d^3 * d^3 * d^3) or O(d^9) or O(v^3) . This is some clown-world-tier complexity and should not find its way into any serious implementation. Due to its simplicity, it could be used to verify the correctness of more complex approaches though.","title":"Absolutely Stupid Method"},{"location":"cuboid_extraction.html#xyz-merge","text":"best possible complexity not optimal An XYZ-merge works by merging all neighboring voxels into lines on one axis first. Then neighboring lines are merged into planes on the second axis. Then neighboring planes are merged into cuboids on the last axis. The results are not optimal and may differ depending on the order of axes. At worst, we can't merge anything and thus iterate over every voxel in each of the steps once. The complexity is O(d^3 * 3) = O(d^3) or O(v) .","title":"XYZ-Merge"},{"location":"cuboid_extraction.html#heightmap-based-approaches","text":"All following approaches are based on the idea, that we can simplify this problem to a 2-dimensional one. One axis is being used as a height-axis, ideally the smallest one if the model is not cubical. We then use a sweeping-plane approach where at each coordinate, we draw a heightmap which extends into our chosen direction. Finding the largest volume in the model is reduced to finding the maximum of the largest volumes for all heightmaps.","title":"Heightmap-Based Approaches"},{"location":"cuboid_extraction.html#naive-method","text":"high complexity optimal results The naive method simply to iterate over all heights. We then require O(d^2 * d^2) complexity where one d^2 represents the size of the base and the other d^2 represents the amount of pairs of heights. This would be O(v^(4/3)) , which is not as bad as the stupid approach but still polynomial.","title":"Naive Method"},{"location":"cuboid_extraction.html#modified-kadanes-algorithm","text":"THIS SECTION IS WIP - complexity yet unknown - hopefully optimal results This can be done by using a sweeping-plane algorithm, where each plane is a heightmap which encodes how many voxels extrude upwards, starting from each one of its pixels. The problem is thus reduced to finding the greatest cuboid on a heightmap, which can be done somewhat efficiently using Kadane's algorithm, where any `0` is treated as negative infinity and the heights are cut-off at some maximum height dynamically during the search. For a `n*n` map, Kadane's algorithm has a complexity of `O(n^3)` or `O(n)` per pixel. Whether this is usable has to be investigated further.","title":"Modified Kadane's-Algorithm"},{"location":"cuboid_extraction.html#golden-section-search-not-sure-if-this-actually-works","text":"best known complexity optimal We can achieve logarithmic complexity per voxel. The approach to this is very similar to the naive method, but based on a crucial observation. The maximum volumes at each minimum height form a unimodal function. Our search is still based on finding the greatest are of 1 in a bitmap, but we choose our heights in a smarter way instead of iterating over all heights. Consider a histogram such as: ^ 7| # 6|# # # 5|# # ### # # 4|# # ### ## ### ## # 3|# ## ### ### ### ####### ## 2|#### ### ### #### ### ####### ### 1|##### #### ### ##### ############### #### -+----------------------------------------------> |0123456789abcdefghijklmnopqrstuvwxyzABCDEFGH The greatest areas for each height are: 7: 07 (3:1, 3:7) 6: 07 (3:1, 3:7) found by extruding (3:1, 3:6) >>5: 15 (8:1, a:5) >>4: 15 (8:1, a:5) >>>>3: 21 (w:1, C:3) >>>>2: 21 (w:1, C:3) >>1: 15 (o:1, C:1) Note that the maximum volumes at each height only form a unimodal function if we extrude our found area as far up as possible. Searching for the maximum or minimum of a unimodal function can be done with logarithmic complexity. The final complexity would thus be O(n^2 * log_2 n) or O(log_2 n) per pixel.","title":"Golden-Section-Search (not sure if this actually works)"},{"location":"properties.html","text":"Properties of Voxel Models The cornerstone of compression is discovering and eliminating redundancies in data. We must first recognize these redundancies by observing the nature of voxel models. There are plenty of properties which we can exploit, as described in the following sections. Geometric/Spatial Locality Normalized 3D Perlin Noise with a threshold of 0.4 Voxels rarely come alone. All 3D models will typically take some distinguished shape. There will be large areas with filled geometry (voxels) and large empty areas, air , or void . In artistic circles these are often described as negative space . Uniform voxel noise with a probability of 0.1 that a voxel is set The only kind of model which has no spatial locality whatsoever would be uniformly distributed noise. (see above) These kinds of models are the absolute exception and are rarely if ever seen. Conclusion Compression schemes have to be able to describe large volumes of positive and negative space efficiently. Color/Attribute Locality \"Ragged Cluster\" model visualized in Magica Voxel Color and other attributes such as normals correlate with geometry. This means that the color distance between neighboring voxels is significantly lower than the distance of random points in RGB space. In fact, this correlation is dramatically high. For the above model, the average euclidean distance of any unique pair of neighboring voxel colors (0, 0, 0) \\le (r, g, b) \\le (255, 255, 255) is 8.56 . Normalized for positions in a unit cube instead this is 0.03357 . For uniform random colors, this distance would be equal to Robbin's Constant which is roughly equal to 0.66170 , almost 20 times higher. Conclusion Compression schemes should make use of geometric encoding and attach attributes on top of that. Alternatively voxel compression schemes should encode attributes in a fashion very similar to geometry. Color Sparsity While in principle, plenty of software supports 24-bit True Color RGB or even wider standards, only a small amount of colors are actually used. Especially in the field of Voxel Art , the 3D-equivalent to Pixel Art , small palettes are used. To name an example, Magica Voxel supports 255 unique colors but each individual color is a 24-bit True Color. Conclusion Compression schemes should be aware of potentially very low color counts and exploit these by building a palette. A palette will not always be beneficial since in principle, any amount of colors should be present. However, in many cases this will be beneficial.","title":"Properties of Voxel Models"},{"location":"properties.html#properties-of-voxel-models","text":"The cornerstone of compression is discovering and eliminating redundancies in data. We must first recognize these redundancies by observing the nature of voxel models. There are plenty of properties which we can exploit, as described in the following sections.","title":"Properties of Voxel Models"},{"location":"properties.html#geometricspatial-locality","text":"Normalized 3D Perlin Noise with a threshold of 0.4 Voxels rarely come alone. All 3D models will typically take some distinguished shape. There will be large areas with filled geometry (voxels) and large empty areas, air , or void . In artistic circles these are often described as negative space . Uniform voxel noise with a probability of 0.1 that a voxel is set The only kind of model which has no spatial locality whatsoever would be uniformly distributed noise. (see above) These kinds of models are the absolute exception and are rarely if ever seen.","title":"Geometric/Spatial Locality"},{"location":"properties.html#colorattribute-locality","text":"\"Ragged Cluster\" model visualized in Magica Voxel Color and other attributes such as normals correlate with geometry. This means that the color distance between neighboring voxels is significantly lower than the distance of random points in RGB space. In fact, this correlation is dramatically high. For the above model, the average euclidean distance of any unique pair of neighboring voxel colors (0, 0, 0) \\le (r, g, b) \\le (255, 255, 255) is 8.56 . Normalized for positions in a unit cube instead this is 0.03357 . For uniform random colors, this distance would be equal to Robbin's Constant which is roughly equal to 0.66170 , almost 20 times higher.","title":"Color/Attribute Locality"},{"location":"properties.html#color-sparsity","text":"While in principle, plenty of software supports 24-bit True Color RGB or even wider standards, only a small amount of colors are actually used. Especially in the field of Voxel Art , the 3D-equivalent to Pixel Art , small palettes are used. To name an example, Magica Voxel supports 255 unique colors but each individual color is a 24-bit True Color.","title":"Color Sparsity"},{"location":"statistical_tests.html","text":"Statistical Tests to Determine Voxel Model Properties The following list should give an overview over possible properties which voxel models are suspected to have. A description of an algorithm and/or pseudocude is used to provide a potential test. Spatial Locality of Different Methods of Iteration Over Voxel Models There are various different ways of iterating over a voxel model. We will consider only two or three, which are relevant to us. Nested Loop Iteration This is the traditional method of iterating over multi-dimensional arrays. char data[SIZE_X][SIZE_Y][SIZE_Z]; for (size_t x = 0; x < SIZE_X; ++x) for (size_t y = 0; y < SIZE_Y; ++y) for (size_t z = 0; z < SIZE_Z; ++z) data[x][y][z]; It is extremely efficient for traversing memory because it seemlessly iterates over memory locations, as long as the axes of iteration are in the right order. The distances between two positions are at best 1 and at worst SIZE_X + SIZEY . However, the spatial locality is much worse when looking at more than one position. For say, 10 positions the distance between the first and last will be 10, which is comparably far. Z-Order Iteration Z-Order iteration traverses space with much higher locality. It works by traversing all nodes of an octree depth-first. 3D-Vectors can be converted into \"octree positions\" by interleaving the bits of the coordinates into a single number. char data[SIZE_X][SIZE_Y][SIZE_Z]; for (size_t x = 0; x < SIZE_X; ++x) for (size_t y = 0; y < SIZE_Y; ++y) for (size_t z = 0; z < SIZE_Z; ++z) data[interleave_bits(x, y, z)]; ) Hilbert-Curve Iteration Hilbert-Curves have even higher locality than Z-Order iterations, but are considerably better in locality. See https://slideplayer.com/slide/3370293/ for construction. In short, space is filled in the following order, which is a Gray Code. [[0, 0, 0], [0, 0, 1], [0, 1, 1], [0, 1, 0], [1, 1, 0], [1, 1, 1], [1, 0, 1], [1, 0, 0]] The entry direction for one of these pieces is +Z and the exit-direction is -Y. This pattern is repeated recursively, but the smaller building blocks have to be mirrored and rotated to fit together seemlessly into a larger block. The tremendously useful property here is that the distance between two points in the iteration is at most 1 . Testing Spatial Locality Spatial locality can simply be tested by comparing the average distance between each pair of points, triple of points, etc. within an iteration. When more than two points are tested, the distances of each unique pair of points can be summed up or averaged. Hilber-Curve Iteration is expected to deliver the best results on most, if not all scales. Correlation of Color and Geometry Deltas It is expected that color correlates with positions. This means that positions which are close to each other should also have similar colors. To verify this property, each unique pair of points can be iterated over. The euclidean distance in geometry-space (scaled down to a unit-cube) as well as the euclidean distance in the RGB color space should be plotted. The correlation can then be calculated from all data entries. struct Entry { double geoDistance; double colDistance; }; struct ColoredPoint { vec3 geo; vec3 col; } std::vector<double[2]> entries; for (const std::pair<ColoredPoint, ColoredPoint> &pair : allPoints) { double geoDistance = pair.first.geo.distanceTo(pair.second.geo); double colDistance = pair.first.col.distanceTo(pair.second.col); entries.emplace_back(geoDistance, colDistance); }","title":"Statistical Tests to Determine Voxel Model Properties"},{"location":"statistical_tests.html#statistical-tests-to-determine-voxel-model-properties","text":"The following list should give an overview over possible properties which voxel models are suspected to have. A description of an algorithm and/or pseudocude is used to provide a potential test.","title":"Statistical Tests to Determine Voxel Model Properties"},{"location":"statistical_tests.html#spatial-locality-of-different-methods-of-iteration-over-voxel-models","text":"There are various different ways of iterating over a voxel model. We will consider only two or three, which are relevant to us.","title":"Spatial Locality of Different Methods of Iteration Over Voxel Models"},{"location":"statistical_tests.html#correlation-of-color-and-geometry-deltas","text":"It is expected that color correlates with positions. This means that positions which are close to each other should also have similar colors. To verify this property, each unique pair of points can be iterated over. The euclidean distance in geometry-space (scaled down to a unit-cube) as well as the euclidean distance in the RGB color space should be plotted. The correlation can then be calculated from all data entries. struct Entry { double geoDistance; double colDistance; }; struct ColoredPoint { vec3 geo; vec3 col; } std::vector<double[2]> entries; for (const std::pair<ColoredPoint, ColoredPoint> &pair : allPoints) { double geoDistance = pair.first.geo.distanceTo(pair.second.geo); double colDistance = pair.first.col.distanceTo(pair.second.col); entries.emplace_back(geoDistance, colDistance); }","title":"Correlation of Color and Geometry Deltas"},{"location":"uncompressed.html","text":"Uncompressed Format The uncompressed format used for reference in this project is a 32-bit list of voxels. In this case a voxel is a triple of coordinates and an ARGB integer, meaning that voxels can be partially transparent. Example Implementation Here is a simple example implementation of a 32-bit voxel list in C++. struct voxel { int32_t x; int32_t y; int32_t z; uint8_t a; uint8_t r; uint8_t g; uint8_t b; } std::vector<voxel> voxel_list; Justification - Voxel Arrays vs. Voxel Lists There are at least two popular methods of representing voxels in an uncompressed way: 3D-array of colors, aka. voxel array array of coordinate/color pairs, aka. voxel list The first method is often used in software on a small scale. Arrays allow for random access in O(1) . They are also used in other research, such as High Resolution Sparse Voxel DAGs . Despite its popularity, arrays are unsuitable for measuring compression ratios because the entropy of the voxel data poorly correlates with the size of this representation. For instance, two voxels at (1, 1, 1) and (1000, 1000, 1000) require 1 gigavoxel of space due to all the empty space between the two voxels. However, in the best case where all voxels are present, this method has only constant overhead: that which is necessary to store the dimensions of the array. The second representation -which is the one used here- will require space that linearly scales with the amount of non-empty voxels. Note that in the worst case, this could require four times the space of the first method due to the coordinates being stored explicitly. This kind of overhead is still preferable to the potential (near) complete waste of space of the first method. To summarize, here is a quick overview: Voxel Array Voxel List worst case overhead (space) O(\\infty) 1 O(4n) 2 best case overhead (space) \\Omega(1) \\Omega(4n) 2 random access (time) O(1) O(n) Serialization Serialization of the VL32 representation is trivial because the data structure in memory is similar, if not identical to its serialized counterpart. To serialize it, we must simply write each element of the array from first to last to disk. Unbounded because two voxels can be infinitely far apart, creating infinite wasted space / overhead \u21a9 The constant factor of 4 is irrelevant in this notation, but illustrates the extent of the overhead \u21a9 \u21a9","title":"Uncompressed Format"},{"location":"uncompressed.html#uncompressed-format","text":"The uncompressed format used for reference in this project is a 32-bit list of voxels. In this case a voxel is a triple of coordinates and an ARGB integer, meaning that voxels can be partially transparent.","title":"Uncompressed Format"},{"location":"uncompressed.html#example-implementation","text":"Here is a simple example implementation of a 32-bit voxel list in C++. struct voxel { int32_t x; int32_t y; int32_t z; uint8_t a; uint8_t r; uint8_t g; uint8_t b; } std::vector<voxel> voxel_list;","title":"Example Implementation"},{"location":"uncompressed.html#justification-voxel-arrays-vs-voxel-lists","text":"There are at least two popular methods of representing voxels in an uncompressed way: 3D-array of colors, aka. voxel array array of coordinate/color pairs, aka. voxel list The first method is often used in software on a small scale. Arrays allow for random access in O(1) . They are also used in other research, such as High Resolution Sparse Voxel DAGs . Despite its popularity, arrays are unsuitable for measuring compression ratios because the entropy of the voxel data poorly correlates with the size of this representation. For instance, two voxels at (1, 1, 1) and (1000, 1000, 1000) require 1 gigavoxel of space due to all the empty space between the two voxels. However, in the best case where all voxels are present, this method has only constant overhead: that which is necessary to store the dimensions of the array. The second representation -which is the one used here- will require space that linearly scales with the amount of non-empty voxels. Note that in the worst case, this could require four times the space of the first method due to the coordinates being stored explicitly. This kind of overhead is still preferable to the potential (near) complete waste of space of the first method. To summarize, here is a quick overview: Voxel Array Voxel List worst case overhead (space) O(\\infty) 1 O(4n) 2 best case overhead (space) \\Omega(1) \\Omega(4n) 2 random access (time) O(1) O(n)","title":"Justification - Voxel Arrays vs. Voxel Lists"},{"location":"uncompressed.html#serialization","text":"Serialization of the VL32 representation is trivial because the data structure in memory is similar, if not identical to its serialized counterpart. To serialize it, we must simply write each element of the array from first to last to disk. Unbounded because two voxels can be infinitely far apart, creating infinite wasted space / overhead \u21a9 The constant factor of 4 is irrelevant in this notation, but illustrates the extent of the overhead \u21a9 \u21a9","title":"Serialization"},{"location":"dag/dag.html","text":"Directed Acyclic Graph Figure 1: A Directed Acyclic Graph Directed Acyclic Graphs (DAGs) for voxel encoding build on the idea of octrees. However, they allow nodes to reference the branches of other nodes. How this may work can be seen in Figure 1 . The second branch of the root node stores a +4 . This +4 points four nodes ahead, on the same level in the graph and tells the decoder \"this node has the same content\" as the node four positions ahead. It is a four because there are three zero bits in the root node between this node and the one it points to.","title":"Directed Acyclic Graph"},{"location":"dag/dag.html#directed-acyclic-graph","text":"Figure 1: A Directed Acyclic Graph Directed Acyclic Graphs (DAGs) for voxel encoding build on the idea of octrees. However, they allow nodes to reference the branches of other nodes. How this may work can be seen in Figure 1 . The second branch of the root node stores a +4 . This +4 points four nodes ahead, on the same level in the graph and tells the decoder \"this node has the same content\" as the node four positions ahead. It is a four because there are three zero bits in the root node between this node and the one it points to.","title":"Directed Acyclic Graph"},{"location":"file_formats/structlang.html","text":"Structure Language (StructLang) StructLang is a data specification language used within the scope of this project. It uses a syntax similar to Rust and C to mathematically specify a syntax for binary data. Comments Comments begin with a // and continue until the end of the line. Alternatively, a comment block which begins with /* and ends with */ can be used. Settings Some global settings may be necessary to write a proper specification. Syntax set = \"set\" identifier \"=\" value; Examples set byte_bits = 32 set default_byte_order = big_endian set default_signed_representation = twos_complement List of Settings Identifier Type Description byte_bits unsigned integer number of bits in one byte default_byte_order modifier default byte order (endianness) of types default_signed_representation modifier default representation of signed integers Type Definitions Definitions define data types. Syntax def = \"def\" type \"=\" {modifier} base_type; type is the identifier of the type modifier a type modifier base_type the base type which this type is specializing Example def i32 = signed 32_bit twos_complement integer def u8 = unsigned 8_bit integer def string = null_terminated u8[] Arrays Arrays are blocks of one type which are stored using a single variable. Syntax array_type = type \"[\" size | \"\" \"]\"; Examples u8[4096] buffer // a constant-sized array u32 size u8[size] var_buffer // a variable-sized array u8[] endless_buffer // an array which extends until the end of the data Structures Defines a structure of other types. These types can also be structures. The first (uppermost) type in a struct is also the first element in the serialized data. Syntax struct = \"struct\" identifier \"{\" type identifier ... \"}\"; Example struct Color { u8 red u8 green u8 blue } Enumerations Defines a list of constants of some type. Syntax enum = \"enum\", identifier, \":\", type, \"{\", {identifier, \"=\", value}, \"}\"; Examples enum Size : string { SMALL = \"S\" MEDIUM = \"M\" LARGE = \"L\" EXTRA_LARGE = \"XL\" } enum ArgbColor : u32 { RED = 0xffff0000 GREEN = 0xff00ff00 BLUE = 0xff0000ff } Templated Structures Sometimes a structure depends on content found in a header or other structures. Templates can change how a structure is laid out using variables. Syntax struct = \"struct\" type \"<\" type {identifier \",\"} \">\" \"{\" ... \"}\"; Example struct varint<u8 bits> { if bits == 8 { u8 data } else if bits == 16 { u16 data } else if bits == 32 { u32 data } else { error \"Invalid bits variable\" } }","title":"Structure Language (StructLang)"},{"location":"file_formats/structlang.html#structure-language-structlang","text":"StructLang is a data specification language used within the scope of this project. It uses a syntax similar to Rust and C to mathematically specify a syntax for binary data.","title":"Structure Language (StructLang)"},{"location":"file_formats/structlang.html#comments","text":"Comments begin with a // and continue until the end of the line. Alternatively, a comment block which begins with /* and ends with */ can be used.","title":"Comments"},{"location":"file_formats/structlang.html#settings","text":"Some global settings may be necessary to write a proper specification.","title":"Settings"},{"location":"file_formats/structlang.html#type-definitions","text":"Definitions define data types.","title":"Type Definitions"},{"location":"file_formats/structlang.html#arrays","text":"Arrays are blocks of one type which are stored using a single variable.","title":"Arrays"},{"location":"file_formats/structlang.html#structures","text":"Defines a structure of other types. These types can also be structures. The first (uppermost) type in a struct is also the first element in the serialized data.","title":"Structures"},{"location":"file_formats/structlang.html#enumerations","text":"Defines a list of constants of some type.","title":"Enumerations"},{"location":"file_formats/structlang.html#templated-structures","text":"Sometimes a structure depends on content found in a header or other structures. Templates can change how a structure is laid out using variables.","title":"Templated Structures"},{"location":"file_formats/vl32.html","text":"32-Bit Voxel List (VL32) VL32 is a simple, binary, intermediate file format used in the code of this research project. Extension: .vl32 Media Type: model/x-vl32 Specification VL32 can be specified in only a few lines of StructLang: def u8 = unsigned 8_bit integer def i32 = big_endian twos_complement 32_bit integer struct main { voxel[] voxels } struct voxel { i32 x i32 y i32 z argb32 color } struct argb32 { u8 alpha u8 red u8 green u8 blue } Voxels with a color that has an alpha of zero are treated as void. Use Case VL32 is largely used for benchmarking compression efficiency. Any compression effort should yield significantly higher entropy than this format. One of the significant advantages is that there doesn't need to be any header information. When reading, voxels are simply loaded until the EOF is reached. Also there is no difference in data size between the file encoded on disk and its typical in-memory representation, such as a std::vector of voxels.","title":"32-Bit Voxel List (VL32)"},{"location":"file_formats/vl32.html#32-bit-voxel-list-vl32","text":"VL32 is a simple, binary, intermediate file format used in the code of this research project. Extension: .vl32 Media Type: model/x-vl32","title":"32-Bit Voxel List (VL32)"},{"location":"file_formats/vl32.html#specification","text":"VL32 can be specified in only a few lines of StructLang: def u8 = unsigned 8_bit integer def i32 = big_endian twos_complement 32_bit integer struct main { voxel[] voxels } struct voxel { i32 x i32 y i32 z argb32 color } struct argb32 { u8 alpha u8 red u8 green u8 blue } Voxels with a color that has an alpha of zero are treated as void.","title":"Specification"},{"location":"file_formats/vl32.html#use-case","text":"VL32 is largely used for benchmarking compression efficiency. Any compression effort should yield significantly higher entropy than this format. One of the significant advantages is that there doesn't need to be any header information. When reading, voxels are simply loaded until the EOF is reached. Also there is no difference in data size between the file encoded on disk and its typical in-memory representation, such as a std::vector of voxels.","title":"Use Case"},{"location":"file_formats/vlascii.html","text":"ASCII Voxel List (VLASCII) VLASCII is a simple, text-based, intermediate file format used in the code of this research project. Extension: .vlascii Media Type: model/x-vlascii Specification Here the specification, in EBNF . vlascii = { line }; (* lines can be empty *) line = [comment | voxel], {\" \"}, nl; (* trailing space allowed *) comment = \"#\", { ?any character? } - nl; voxel = xyz, space, color, space; xyz = integer, space, integer, space, integer; color = (h, h, h, h, h, h) | (h, h, h); h = ?hexadecimal digit?; integer = [\"-\"], ?decimal digit?, { ?decimal digit? }; nl = ?newline character?; space = \" \", { \" \" }; Example # red voxel at 0, 0, 0 0 0 0 ff0000 # white voxel at 0, 1, 0 0 1 0 fff # black voxel at 0, 0, 1 0 0 1 000","title":"ASCII Voxel List (VLASCII)"},{"location":"file_formats/vlascii.html#ascii-voxel-list-vlascii","text":"VLASCII is a simple, text-based, intermediate file format used in the code of this research project. Extension: .vlascii Media Type: model/x-vlascii","title":"ASCII Voxel List (VLASCII)"},{"location":"file_formats/vlascii.html#specification","text":"Here the specification, in EBNF . vlascii = { line }; (* lines can be empty *) line = [comment | voxel], {\" \"}, nl; (* trailing space allowed *) comment = \"#\", { ?any character? } - nl; voxel = xyz, space, color, space; xyz = integer, space, integer, space, integer; color = (h, h, h, h, h, h) | (h, h, h); h = ?hexadecimal digit?; integer = [\"-\"], ?decimal digit?, { ?decimal digit? }; nl = ?newline character?; space = \" \", { \" \" };","title":"Specification"},{"location":"file_formats/vlascii.html#example","text":"# red voxel at 0, 0, 0 0 0 0 ff0000 # white voxel at 0, 1, 0 0 1 0 fff # black voxel at 0, 0, 1 0 0 1 000","title":"Example"},{"location":"flvc/flvc.html","text":"Free Lossless Voxel Compression (FLVC) FLVC is the file format produced based on the results of the presented research. Extension: .flvc Media Type: model/x-flvc Magic Bytes: \"\\xff\\x11\\x33\\xccflvc\" = ff 11 33 cc 66 6c 76 63 Container Format Specification // TYPE DEFINITIONS ============================================================ type u8 = unsigned integer<8> type u16 = little_endian unsigned integer<16> type u32 = little_endian unsigned integer<32> type u64 = little_endian unsigned integer<64> type i8 = twos_complement integer<8> type i16 = little_endian twos_complement integer<16> type i32 = little_endian twos_complement integer<32> type i64 = little_endian twos_complement integer<64> // CONSTANTS =================================================================== enum attribute_modifier : u16 { // this attribute has no spatial locality // should be used for random unique ids etc. NON_LOCAL = 1 // only applicable to the \"color\" attribute // whatever color model is used, alpha will be appended to it last // ARGB is the default, in which alpha is first ALPHA_LAST = 2 } enum attribute_type : u8 { // 8-bit unsigned integer were any nonzero value is interpreted as true BOOL = 0x01 INT_8 = 0x11 // see i8 type INT_16 = 0x12 // see i16 type INT_32 = 0x14 // see i32 type INT_64 = 0x18 // see i64 type UINT_8 = 0x21 // see u8 type UINT_16 = 0x22 // see u16 type UINT_32 = 0x24 // see u32 type UINT_64 = 0x28 // see u64 type FLOAT_32 = 0x41 // 32-bit floating point FLOAT_64 = 0x42 // 64-bit floating point } enum builtin_identifiers : string8 { // attributes will be used as the color of the voxel COLOR = \"color\" // attributes will be used as the normal of the voxel NORMAL = \"normal\" } // HELPER STRUCTS ============================================================== struct string8 { nonzero u8 size // ASCII-encoded max<127> u8[size] content } // HEADER ====================================================================== struct main { // 8 bytes of file magic u64 magic = 0xff11_33cc_666c_7663 // major version and minor version, currently 0.1 u8 version_major = 0 u8 version_minor = 1 // The offset of the volume from the origin i32[3] volume_offset // The exact dimensions of the voxel volume. // The SVO has to contain these dimensions, but may be larger internally. // (SVOs can only have power-of-two dimensions) u32[3] volume_size // 1 if the volume contains no voxels, else 0 u8 empty // pads the fixed-size header content to a total length of 40 bytes u8[5] padding = \"(def)\" u16 definitions_size attribute_definition[definitions_size] definitions // A visual separator between header and content, visible when opening files // in a hex editor. // Also useful to detect reading past the header in implementations. u8 reserved = '|' // Only store a zlib-encoded stream for non-empty SVOs. // Emptyness is indicated by the \"empty\" variable above. if (!header.empty) { // (extern because can't be fully defined in StructLang) extern zlib_stream content } } struct attribute_definition { // unique identifier of the attribute // certain values such as \"color\" are builtin and reserved // (see enum builtin_identifiers) string8 identifier // type of the attribute attribute_type atype // If zero, the type is a variable sized-array. // If not zero, this indicates that the type is a fixed-size array-type or // vector-type. // The position attribute must have a cardinality of 3! u8 cardinality // bitmap storing all modifiers u16 modifiers } header.size is not strictly necessary for decoding, but it can be helpful when converting directly to a 3D array or other format which requires a size upon construction. The attribute definitions allow us to practically store any data we want inside the format. For example, store an RGB24 value: modifiers = 0 atype = UINT_8 cardinality = 3 identifier = \"color\" The actual content is too complex to be expressed in StructLang.","title":"Free Lossless Voxel Compression (FLVC)"},{"location":"flvc/flvc.html#free-lossless-voxel-compression-flvc","text":"FLVC is the file format produced based on the results of the presented research. Extension: .flvc Media Type: model/x-flvc Magic Bytes: \"\\xff\\x11\\x33\\xccflvc\" = ff 11 33 cc 66 6c 76 63","title":"Free Lossless Voxel Compression (FLVC)"},{"location":"flvc/flvc.html#container-format-specification","text":"// TYPE DEFINITIONS ============================================================ type u8 = unsigned integer<8> type u16 = little_endian unsigned integer<16> type u32 = little_endian unsigned integer<32> type u64 = little_endian unsigned integer<64> type i8 = twos_complement integer<8> type i16 = little_endian twos_complement integer<16> type i32 = little_endian twos_complement integer<32> type i64 = little_endian twos_complement integer<64> // CONSTANTS =================================================================== enum attribute_modifier : u16 { // this attribute has no spatial locality // should be used for random unique ids etc. NON_LOCAL = 1 // only applicable to the \"color\" attribute // whatever color model is used, alpha will be appended to it last // ARGB is the default, in which alpha is first ALPHA_LAST = 2 } enum attribute_type : u8 { // 8-bit unsigned integer were any nonzero value is interpreted as true BOOL = 0x01 INT_8 = 0x11 // see i8 type INT_16 = 0x12 // see i16 type INT_32 = 0x14 // see i32 type INT_64 = 0x18 // see i64 type UINT_8 = 0x21 // see u8 type UINT_16 = 0x22 // see u16 type UINT_32 = 0x24 // see u32 type UINT_64 = 0x28 // see u64 type FLOAT_32 = 0x41 // 32-bit floating point FLOAT_64 = 0x42 // 64-bit floating point } enum builtin_identifiers : string8 { // attributes will be used as the color of the voxel COLOR = \"color\" // attributes will be used as the normal of the voxel NORMAL = \"normal\" } // HELPER STRUCTS ============================================================== struct string8 { nonzero u8 size // ASCII-encoded max<127> u8[size] content } // HEADER ====================================================================== struct main { // 8 bytes of file magic u64 magic = 0xff11_33cc_666c_7663 // major version and minor version, currently 0.1 u8 version_major = 0 u8 version_minor = 1 // The offset of the volume from the origin i32[3] volume_offset // The exact dimensions of the voxel volume. // The SVO has to contain these dimensions, but may be larger internally. // (SVOs can only have power-of-two dimensions) u32[3] volume_size // 1 if the volume contains no voxels, else 0 u8 empty // pads the fixed-size header content to a total length of 40 bytes u8[5] padding = \"(def)\" u16 definitions_size attribute_definition[definitions_size] definitions // A visual separator between header and content, visible when opening files // in a hex editor. // Also useful to detect reading past the header in implementations. u8 reserved = '|' // Only store a zlib-encoded stream for non-empty SVOs. // Emptyness is indicated by the \"empty\" variable above. if (!header.empty) { // (extern because can't be fully defined in StructLang) extern zlib_stream content } } struct attribute_definition { // unique identifier of the attribute // certain values such as \"color\" are builtin and reserved // (see enum builtin_identifiers) string8 identifier // type of the attribute attribute_type atype // If zero, the type is a variable sized-array. // If not zero, this indicates that the type is a fixed-size array-type or // vector-type. // The position attribute must have a cardinality of 3! u8 cardinality // bitmap storing all modifiers u16 modifiers } header.size is not strictly necessary for decoding, but it can be helpful when converting directly to a 3D array or other format which requires a size upon construction. The attribute definitions allow us to practically store any data we want inside the format. For example, store an RGB24 value: modifiers = 0 atype = UINT_8 cardinality = 3 identifier = \"color\" The actual content is too complex to be expressed in StructLang.","title":"Container Format Specification"},{"location":"flvc/flvc_optimizations.html","text":"FLVC Data Stream The FLVC file format consists of a container format and a zlib-compressed data stream as explained in FLVC . The data stream stores a Sparse Voxel Octree, optimized in multiple ways. Due to the FLVC format allowing arbitrary attributes, the exact structure of the data stream is also variable. Attribute definitions in the container specify the structure of the attribute data per voxel. This structural information is crucial for certain optimizations performed in the FLVC codec. General Structure Generally, the FLVC data stream consists of nodes of an SVO, ordered in Accelerated Depth First order. For the FLVC data stream, this order has the advantage that multiple child nodes can be compressed and decompressed at the same time, compared to Depth First order, where only one node can be read at a time. Accelerated Depth First order allows for optimizations that rely on all neighbor nodes being known to the decoder at the same time. Each node is structured as illustrated under SVO Serialization . There are two types of nodes: Leaf nodes store a child mask of voxels and attribute data for each voxel. Branch nodes only store a mask of other nodes, which can be either leafs or branches . Without any optimization steps, a typical data stream with mask m one two-byte attribute a would look like: (m_0, m_1, m_2, m_3, a^\\text{lo}_3, a^\\text{hi}_3, \\ldots, m_n , a^\\text{lo}_n, a^\\text{hi}_n) In this example, 0..2 are branch nodes, 3 is the first leaf node. FLVC data streams encode all attributes in little-endian byte order. This byte order is not only the standard for x86 architectures, but is also easier to perform optimization steps such as bitwise interleaving on. Encoding and Decoding Complexity The encoder needs to construct the SVO in its entirety before writing anything to disk. Even the very last voxel which is read from the input could change the SVO structurally. For example, a voxel that lies outside the current capacity of the SVO will require (unilateral) growth of the tree structure, meaning that the tree receives a new root node. Depth-first encoded trees can be read using only a stack, resulting in O(\\log{v}) space complexity for the decoder. Accelerated depth-first requires O(b \\log_b{v} ) space complexity, where b is the branching factor of the tree, which is eight in this case. FLVC only allows 21 SVO layers because the Octree Node Index needs to fit a 64-bit integer. So in practice, the decoder can simply allocate 8 \\cdot 21 \\cdot x bytes of memory, where x is the size of attribute data, child masks, and internal data stored per node. In summary: Complexity Encoder Decoder Space O(v) O(8 \\log_8{v}) Time O(v) O(v) In practice, decoding is about twice as fast as encoding due to the lower memory requirements. Computationally, all following optimizations have linear complexity. Some steps require propagating information from the bottom to the top of the SVO, but as explained in SVO - Extreme Cases And Limits , this results in an additional cost of at worst \\frac{1}{7} . Optimizations Overview There are three layers of optimizations applied to the SVO before writing to disk: SVO Optimizations Delta Coding Attribute De-Interleaving Bitwise Interleaving The first step consists of the two SVO optimizations Single-Octant Optimization and Complete Branch Trimming . All three further optimizations require all children to be available to the encoder/decoder. These optimizations steps do not decrease the size of the data stream whatsoever. Delta Coding even increases it. However, they turn geometric redundancy into bitwise redundancy . In short, this means that redundancies such as clustering of voxels in one region of space, as well as the clustering of attributes are converted into a redundancy at the bit level. This can then be exploited by an entropy coder. In the case of FLVC, the whole data stream is compressed using zlib. Delta Coding Idea Delta Coding is a popular method of eliminating redundancy from data where the numerical difference between consecutive elements most often falls into a limited range. Take the following example: \\begin{align} n_i =& (0, 1, 2, 3, 4, 5, ...) \\\\ \\frac{\\Delta n_i}{\\Delta i} =& (1, 1, 1, ...) \\\\ \\frac{\\Delta\\Delta n_i}{\\Delta i} =& (0, 0, 0, ...) \\end{align} Here, it may be obvious to the viewer that the n_i follows a very predictable pattern. However, at the bit level, this sequence would be (00_2, 01_2, 10_2, 11_2, \\ldots) . We can see that there is a lot of variation in the lowest two bits. The first derivative has no variation at all and the second derivative is simply an infinite zero-sequence. In conclusion, we turn our knowledge that the sequence follows a predictable pattern into a bitwise redundancy . FLVC Delta Coding Delta Coding in FLVC takes place between child and parent, meaning that the attributes of the children are converted to a delta from the parent attribute. At this point you may be confused, because parents don't have any attributes. Only voxels, which are the leaf nodes of the tree, have attributes such as color; their parents do not. As part of this optimization step, we must assign values to the parents depending on their children. This occurs in a mipmapping or reduction step where the values of children are propagated towards the root of the tree using the min operator. The parents thus receive the minimum attribute of their children and the children are stored as deltas to this common minimum. Such a reduction step is performed bottom-up for the entire tree by the encoder after the last voxel has been stored in the SVO. This step exploits attribute locality , meaning that voxels close to each other also tend to have similar attributes. When all attributes fall into a similar range, the deltas will all be low as well. Note The reason for using the min operator in particular is that deltas are always representable as unsigned integers without relying on specific overflow semantics. Deltas must be unsigned to avoid problems such as the delta between the 8-bit unsigned integers 0 and 200 not being representable by an 8-bit signed integer. Example // original attribute values of the child nodes child[0] = 200 = 0b1100'1000 child[1] = 205 = 0b1100'1101 child[2] = 210 = 0b1101'0010 child[3] = 215 = 0b1101'0010 // minimum assigned to parent parent = 200 = min(child[0..3]) // deltas to parent computed and stored in each child node child[0] - parent = 0 = 0b0000'0000 child[1] - parent = 5 = 0b0000'1001 child[2] - parent = 10 = 0b0000'1010 child[3] - parent = 15 = 0b0000'1111 As can be seen in this example, the attribute values started out as having differences both in the upper and lower nibble (half-byte). After the optimization step, there is only variation in the lower nibble. The upper nibble is completely zeroed out. Attribute De-Interleaving In the next step, attributes are de-interleaved. Remember, our data stream currently looks like this, where m is a child mask and a is an attribute: (m_0, a_0, m_1, (a_1-a_0), m_2, (a_2-a_0), m_3, (a_3-a_0), \\ldots) Attributes which might have no correlation with each other are stored right next to each other in the stream. For example, high-entropy child masks are stored right next to attributes, which are delta coded and potentially all zeroed out. By de-interleaving these attributes and storing one list per attribute instead, we can bring correlated bytes closer together in the byte stream: (m_0, a_0, m_1, m_2, m_3, (a_1-a_0), (a_2-a_0), (a_3-a_0), \\ldots) We can only do this for at most eight neighboring nodes at a time. In a breadth-first stream, we could also perform this for an entire layer of the tree, but we use Accelerated Depth-First . This optimization is almost trivial computationally, because we can simply create a permutation for each possible number of child nodes for our specific attribute layout. Decoding (interleaving) can be performed just as easily by computing the inverse of the pre-computed permutation. Bitwise Interleaving The motivation of this is to clump together all less significant bits and all more significant bits, per child. After Delta Coding, the more significant bits will often consist of only zeros. Grouping together those bits is helpful to entropy coders, which can then perform RLE or other compression steps. An example of how bit-interleaving works can be found under SVO Optimization . After this final optimization step, the data stream looks as follows: (m_0, a_0, m_1, m_2, m_3, \\text{ileave}(a_1-a_0, a_2-a_0, a_3-a_0), \\ldots) Note Attribute De-Interleaving and Bitwise Interleaving together are actually equivalent to bitwise interleaving of the raw data of each node.","title":"FLVC Data Stream"},{"location":"flvc/flvc_optimizations.html#flvc-data-stream","text":"The FLVC file format consists of a container format and a zlib-compressed data stream as explained in FLVC . The data stream stores a Sparse Voxel Octree, optimized in multiple ways. Due to the FLVC format allowing arbitrary attributes, the exact structure of the data stream is also variable. Attribute definitions in the container specify the structure of the attribute data per voxel. This structural information is crucial for certain optimizations performed in the FLVC codec.","title":"FLVC Data Stream"},{"location":"flvc/flvc_optimizations.html#general-structure","text":"Generally, the FLVC data stream consists of nodes of an SVO, ordered in Accelerated Depth First order. For the FLVC data stream, this order has the advantage that multiple child nodes can be compressed and decompressed at the same time, compared to Depth First order, where only one node can be read at a time. Accelerated Depth First order allows for optimizations that rely on all neighbor nodes being known to the decoder at the same time. Each node is structured as illustrated under SVO Serialization . There are two types of nodes: Leaf nodes store a child mask of voxels and attribute data for each voxel. Branch nodes only store a mask of other nodes, which can be either leafs or branches . Without any optimization steps, a typical data stream with mask m one two-byte attribute a would look like: (m_0, m_1, m_2, m_3, a^\\text{lo}_3, a^\\text{hi}_3, \\ldots, m_n , a^\\text{lo}_n, a^\\text{hi}_n) In this example, 0..2 are branch nodes, 3 is the first leaf node. FLVC data streams encode all attributes in little-endian byte order. This byte order is not only the standard for x86 architectures, but is also easier to perform optimization steps such as bitwise interleaving on.","title":"General Structure"},{"location":"flvc/flvc_optimizations.html#encoding-and-decoding-complexity","text":"The encoder needs to construct the SVO in its entirety before writing anything to disk. Even the very last voxel which is read from the input could change the SVO structurally. For example, a voxel that lies outside the current capacity of the SVO will require (unilateral) growth of the tree structure, meaning that the tree receives a new root node. Depth-first encoded trees can be read using only a stack, resulting in O(\\log{v}) space complexity for the decoder. Accelerated depth-first requires O(b \\log_b{v} ) space complexity, where b is the branching factor of the tree, which is eight in this case. FLVC only allows 21 SVO layers because the Octree Node Index needs to fit a 64-bit integer. So in practice, the decoder can simply allocate 8 \\cdot 21 \\cdot x bytes of memory, where x is the size of attribute data, child masks, and internal data stored per node. In summary: Complexity Encoder Decoder Space O(v) O(8 \\log_8{v}) Time O(v) O(v) In practice, decoding is about twice as fast as encoding due to the lower memory requirements. Computationally, all following optimizations have linear complexity. Some steps require propagating information from the bottom to the top of the SVO, but as explained in SVO - Extreme Cases And Limits , this results in an additional cost of at worst \\frac{1}{7} .","title":"Encoding and Decoding Complexity"},{"location":"flvc/flvc_optimizations.html#optimizations-overview","text":"There are three layers of optimizations applied to the SVO before writing to disk: SVO Optimizations Delta Coding Attribute De-Interleaving Bitwise Interleaving The first step consists of the two SVO optimizations Single-Octant Optimization and Complete Branch Trimming . All three further optimizations require all children to be available to the encoder/decoder. These optimizations steps do not decrease the size of the data stream whatsoever. Delta Coding even increases it. However, they turn geometric redundancy into bitwise redundancy . In short, this means that redundancies such as clustering of voxels in one region of space, as well as the clustering of attributes are converted into a redundancy at the bit level. This can then be exploited by an entropy coder. In the case of FLVC, the whole data stream is compressed using zlib.","title":"Optimizations Overview"},{"location":"flvc/flvc_optimizations.html#delta-coding","text":"","title":"Delta Coding"},{"location":"flvc/flvc_optimizations.html#attribute-de-interleaving","text":"In the next step, attributes are de-interleaved. Remember, our data stream currently looks like this, where m is a child mask and a is an attribute: (m_0, a_0, m_1, (a_1-a_0), m_2, (a_2-a_0), m_3, (a_3-a_0), \\ldots) Attributes which might have no correlation with each other are stored right next to each other in the stream. For example, high-entropy child masks are stored right next to attributes, which are delta coded and potentially all zeroed out. By de-interleaving these attributes and storing one list per attribute instead, we can bring correlated bytes closer together in the byte stream: (m_0, a_0, m_1, m_2, m_3, (a_1-a_0), (a_2-a_0), (a_3-a_0), \\ldots) We can only do this for at most eight neighboring nodes at a time. In a breadth-first stream, we could also perform this for an entire layer of the tree, but we use Accelerated Depth-First . This optimization is almost trivial computationally, because we can simply create a permutation for each possible number of child nodes for our specific attribute layout. Decoding (interleaving) can be performed just as easily by computing the inverse of the pre-computed permutation.","title":"Attribute De-Interleaving"},{"location":"flvc/flvc_optimizations.html#bitwise-interleaving","text":"The motivation of this is to clump together all less significant bits and all more significant bits, per child. After Delta Coding, the more significant bits will often consist of only zeros. Grouping together those bits is helpful to entropy coders, which can then perform RLE or other compression steps. An example of how bit-interleaving works can be found under SVO Optimization . After this final optimization step, the data stream looks as follows: (m_0, a_0, m_1, m_2, m_3, \\text{ileave}(a_1-a_0, a_2-a_0, a_3-a_0), \\ldots) Note Attribute De-Interleaving and Bitwise Interleaving together are actually equivalent to bitwise interleaving of the raw data of each node.","title":"Bitwise Interleaving"},{"location":"flvc/full_example.html","text":"Full Example To better illustrate the functionality of the FLVC codec, we will walk through a full example of encoding a VL32 file using FLVC manually. Our test model will be the following 4x4x4 partially filled cube. The blue voxel is the uppermost point (3,3,3) and, the bottom-right, red voxel is the lowermost point at (0,0,0) . The octant containing gray voxels is completely filled, meaning it consists of 2x2x2 voxels. Figure 1: The test model VL32 Encoding See VL32 Specification . This following block shows a hex-printout of the VL32 file with added comments. Each row is one voxel. Each voxel consist of three big-endian two-complement coordinates followed by ARGB color bytes. // 4 red bottom voxels 00000000 00000000 00000000 ffff000a 00000001 00000000 00000000 ffff000a 00000000 00000000 00000001 ffff000a 00000001 00000000 00000001 ffff000a // 2 orange voxels 00000000 00000001 00000000 ffff8b33 00000001 00000001 00000001 ffff8b33 // 8 gray voxels 00000000 00000000 00000002 ff646464 00000001 00000000 00000002 ff646464 00000000 00000001 00000002 ff646464 00000001 00000001 00000002 ff646464 00000000 00000000 00000003 ff646464 00000001 00000000 00000003 ff646464 00000000 00000001 00000003 ff646464 00000001 00000001 00000003 ff646464 // 1 white voxel 00000003 00000001 00000000 ffffffff // 1 green voxel 00000002 00000002 00000002 ff33ff4c // 1 blue voxel 00000003 00000003 00000003 ff3373ff Conversion to FLVC Step by Step Header We start out by encoding the constant parts of the header: ff1133cc666c7663 // magic bytes 00 01 // version 0.1 feffffff feffffff feffffff // volume_offset = (-2, -2, -2) // keep in mind these are little-endian integers 04000000 04000000 04000000 // volume_size = (4, 4, 4) 00 // empty = false \"(def)\" // five ASCII padding bytes Attribute Definitions Then, we must encode the attribute definitions. There must always be a definition for the position attribute. While the type and modifiers for this attribute have no influence on the rest of the stored data, they are necessary for the FLVC API to know the layout of position triples when encoding and decoding a data stream. 0200 // definitions_size = 2 08 // identifier_length = 8 \"position\" // identifier 14 // type = 0x14 = INT32 03 // cardinality = 3 0000 // no modifiers 05 // identifier_length = 5 \"color\" // identifier 21 // type = 0x21 = UINT8 04 // cardinality = 4 00 // no modifiers \"|\" // end of header, start of data SVO Processing We must first encode our voxel data as an SVO. We won't go into detail how to do this in this chapter. The end result looks as follows: Figure 2: The test model visualized as a graph In this figure, we see the root node connected to four octants. Each octant has up to eight voxels. The topmost bit in each child mask represents the lowermost octant of a node. Note For each connection to a voxel, a separate node in the SVO is stored. Multiple connections to one node are only part of this visualization to save space. There are some SVO optimizations that we have to apply for FLVC but none of them are necessary here: Trimming completely filled branches would have no impact on the gray octant. The root node has more than octant, so the depth can not be reduced further. The next optimization would be delta coding. For that, we must first perform a min mipmapping step. The color attribute has a cardinality of 4. Henceforth, the min is computed for each channel of the colors. We then delta-code the SVO by subtracting children's channels from their parents'. Here is the result: Figure 3: The mipmapped (left) and then delta-coded (right) graph of the model Interleaving Steps Now, we perform attribute de-interleaving and bitwise interleaving . Starting with the red octant: 00_00_00_00 00 00 00 00 00 00 00_00_00_00 --dileave-> 00 00 00 00 00 00 00_00_8b_29 00 00 8b 00 00 8b 00_00_00_00 00 00 29 00 00 29 00_00_00_00 00_00_8b_29 Essentially, we have turned 6 nodes with 4 attributes each into 4 sequences with one byte for each of the 6 nodes. The first de-interleaved row shows the alpha channel, the next is red, etc. Another example, here in the form of the root node, which contains another 4 branch nodes: b7 00_cc_00_00 b7 ff 40 81 ff 00_31_64_5a --dileave-> 00 00 00 00 40 00_cc_ff_f5 cc 31 cc 00 81 00_00_73_42 00 64 ff 73 00 5a f5 42 We repeat this process for all nodes. At this point it becomes abundantly clear what the effect of these steps is. We started out with a graph that contains a variety of different numbers and no obvious pattern, exploitable by an entropy coder. Now e.g. the attribute data for the red block of voxels starts with 18 zeros, and although the effect on the branch node was less severe, we now still have 4 consecutive zeros due to uniformity in the alpha channel. Now, the final step is bit-interleaving . This means that we concatenate the least significant bit of each row, followed by the next most significant bit, etc. This time, we use the green and blue node as an example: 00 00 8c 00 --ileave-> 00 00 00 00 50 40 0a 8a 00 00 00 b3 Also see Bitwise Interleaving for more examples. Serialization We still need to get all of this data into a single, sequential data stream. In the FLVC reference implementation, the interleaving steps are actually done just-in-time when writing a node, not during optimizations. However, this is ultimately the choice of the implementing developer. The order of nodes in FLVC is Accelerated Depth-First . When arranged in this order, the complete data stream looks as follows: 80 FF 33 00 0A 93 00 00 00 00 B7 00 40 81 00 00 00 00 02 55 22 55 CC 46 EC 4E A4 24 46 4E 00 00 00 00 00 00 00 00 00 00 00 00 24 09 90 00 00 90 24 00 90 00 09 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 50 40 0A 8A Compression When finally compressing, we encode the data stream using zlib and append the stream to the header directly after the | character, which separates header from content. 78 DA 6B F8 6F CC C0 35 99 01 08 B6 33 38 34 82 68 A6 50 A5 D0 33 6E 6F FC 96 A8 B8 F9 31 20 01 15 CE 09 0C 0C 13 54 18 26 30 70 32 10 0B 02 1C B8 BA 00 13 57 0B 6C You can access the complete file for your own purposes here: Download","title":"Full Example"},{"location":"flvc/full_example.html#full-example","text":"To better illustrate the functionality of the FLVC codec, we will walk through a full example of encoding a VL32 file using FLVC manually. Our test model will be the following 4x4x4 partially filled cube. The blue voxel is the uppermost point (3,3,3) and, the bottom-right, red voxel is the lowermost point at (0,0,0) . The octant containing gray voxels is completely filled, meaning it consists of 2x2x2 voxels. Figure 1: The test model","title":"Full Example"},{"location":"flvc/full_example.html#vl32-encoding","text":"See VL32 Specification . This following block shows a hex-printout of the VL32 file with added comments. Each row is one voxel. Each voxel consist of three big-endian two-complement coordinates followed by ARGB color bytes. // 4 red bottom voxels 00000000 00000000 00000000 ffff000a 00000001 00000000 00000000 ffff000a 00000000 00000000 00000001 ffff000a 00000001 00000000 00000001 ffff000a // 2 orange voxels 00000000 00000001 00000000 ffff8b33 00000001 00000001 00000001 ffff8b33 // 8 gray voxels 00000000 00000000 00000002 ff646464 00000001 00000000 00000002 ff646464 00000000 00000001 00000002 ff646464 00000001 00000001 00000002 ff646464 00000000 00000000 00000003 ff646464 00000001 00000000 00000003 ff646464 00000000 00000001 00000003 ff646464 00000001 00000001 00000003 ff646464 // 1 white voxel 00000003 00000001 00000000 ffffffff // 1 green voxel 00000002 00000002 00000002 ff33ff4c // 1 blue voxel 00000003 00000003 00000003 ff3373ff","title":"VL32 Encoding"},{"location":"flvc/full_example.html#conversion-to-flvc-step-by-step","text":"","title":"Conversion to FLVC Step by Step"},{"location":"flvc/results.html","text":"FLVC Performance, Conclusions table { margin-bottom: 2em; } td, th { padding: 6px; border: 1px solid black !important; } td { font-family: monospace; text-align: right; } th { background: silver; } Examples In the following section, we will look at three concrete examples of real world performance of the FLVC codec. All test were performed on the ramdisk of a Linux machine (Ryzen 9 3900X, 64 GiB 3200MHz). Spacestation Voxels 923,566 V Raw Size 14,777,056 B Encoded Size (FLVC-6) (3.34%) 500,812 B Encoded Size (FLVC-9) (3.33%) 493,409 B Encoding Duration (FLVC-6) 1679 ms Encoding Duration (FLVC-9) 2130 ms Scrapyard Voxels 24,423,655 V Raw Size (VL32) 390,778,480 B Compressed Size (FLVC-6) (1.71%) 6,682,496 B Compressed Size (FLVC-9) (1.69%) 6,602,502 B Encoding Duration (FLVC-6) 38 s Encoding Duration (FLVC-9) 44 s Mars Heightmap Voxels 101,244,604 V Raw Size 1,619,913,664 B Encoded Size (FLVC-6) (0.90%) 14,616,607 B Encoded Size (FLVC-9) (0.86%) 13,914,269 B Encoding Duration (FLVC-6) 149 s Encoding Duration (FLVC-9) 163 s Conclusions Compression Ratio Free Lossless Voxel Compression delivers impressive results. Even for the smaller and somewhat less redundant models, we need less than one byte per voxel despite starting with 16 bytes per voxel ( x, y, z, argb ). At best, the file size is reduced by over 99%. It must be emphasized that in related work, the reference point is typically a voxel array where only a miniscule fraction consists of voxels. Here, the reference point is a list of voxels with no empty space between. The level of compression has little impact on the size of the produced filed. At best, a 10% reduction ca be seen, at worst, almost none. Time Execution time can be perceived as excessive for very large models. As can be concluded from previous sections, most of this time is spent reading in VL32 data and constructing the SVO. For example, of the 149 seconds to encode the heightmap at level 6, 108 were spent constructing the SVO. This is a problem that can not be easily resolved algorithmically and is inherent to manipulating large quantities of data. Once again, the compression level has comparably little impact: here on the encoding duration. Further Improvements Some compression ratio improvements could be achieved by choosing a different entropy coder. For example, Facebook's Zstandard is a more recent and promising technology than zlib. A slight improvement in compression speed is also likely. However, the biggest bottleneck as of now is the time it takes to construct the SVO while also reading VL32 data. A potential solution would be to use multiple threads: One for reading the VL32 file and one for constructing the SVO. At most, this would reduce the duration of first part of FLVC encoding by 50%. Another potential approach would be to create multiple, independent volumes, which are all encoded by a separate thread. Potentially, separate zlib streams could be created as well to avoid a bottleneck during entropy coding. However, this would require further modifications of the FLVC standard and is not possible merely by optimizing the reference implementation.","title":"FLVC Performance, Conclusions"},{"location":"flvc/results.html#flvc-performance-conclusions","text":"table { margin-bottom: 2em; } td, th { padding: 6px; border: 1px solid black !important; } td { font-family: monospace; text-align: right; } th { background: silver; }","title":"FLVC Performance, Conclusions"},{"location":"flvc/results.html#examples","text":"In the following section, we will look at three concrete examples of real world performance of the FLVC codec. All test were performed on the ramdisk of a Linux machine (Ryzen 9 3900X, 64 GiB 3200MHz).","title":"Examples"},{"location":"flvc/results.html#conclusions","text":"","title":"Conclusions"},{"location":"related/literature.html","text":"References Literature Efficient Sparse Voxel Octrees Authors: Samuli Laine, Tero Karras Publisher: NVIDIA Research Publication Date: 2010-02-01 Published in: ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games (I3D) Links: NVIDIA Research , PDF Last Viewed: 2020-05-04 Geometry and Attribute Compression for Voxel Scenes Authors: Bas Dado, Timothy R. Kol, Pablo Bauszat, Jean-Marc Thiery, Elmar Eisemann Publisher: Delft University of Technology Publication Date: 2015-12-16 Also published in: EUROGRAPHICS 2016, Volume 35 (2016), Number 2 Links: TUDelft , PDF Last Viewed: 2020-05-04 High Resolution Sparse Voxel DAGs Authors: Viktor Ka\u0308mpe, Erik Sintorn, Ulf Assarsson Publisher: Chalmers University of Technology Publication Date: 2013-07 Published in: ACM Transactions on Graphics , Vol 32, No. 4 Links: ACM Digital Library , PDF Last Viewed: 2020-05-04 VOLA: A Compact Volumetric Format for 3D Mapping and Embedded Systems Authors: Jonathan Byrne, Le\u0301onie Buckley, Sam Caulfield, David Moloney Publisher: Advanced Architecture Group, Intel, Ireland Publication Date: 2018-01 Conference: 4th International Conference on Geographical Information Systems Theory, Applications and Management Published in: Proceedings of the 4th International Conference on Geographical Information Systems Theory, Applications and Management - Volume 1: GISTAM Links: Researchgate , PDF Last Viewed: 2020-05-04 Symmetry-aware Sparse Voxel DAGs (SSVDAGs) for compression-domain tracing of high-resolution geometric scenes Authors: Alberto Jaspe Villanueva, Fabio Marton, Enrico Gobbetti, CRS4 Publication Date: 2017-05-12 Published In: Journal of Computer Graphics Techniques (JCGT), vol. 6, no. 2 Links: Computer Graphics Techniques , Full-Text PDF , Low-Resolution PDF , Video Demo A Computer Oriented Geodetic Data Base And a New Technique in File Sequencing Author: G.M. Morton Publication Date: 1996-03-01 Published In: Journal of Computer Graphics Techniques (JCGT), vol. 6, no. 2 Links: PDF \u00dcber die stetige Abbildung einer Linie auf ein Fl\u00e4chenst\u00fcck {abc} Author: David Hilbert Publication Date: 1891-03-04 Published In: Mathematische Annalen, 38th volume, 3rd booklet Links: DigiZeitschriften , PDF Specifications Qubicle File Specification Articles Authors: Minddesk Software GmbH Publication Dates: 2015-10-02, 2016-01-11 Published at: getqubicle.com Links: QEF Spec. , QB Spec. , QBT Spec. Last Viewed: 2020-05-04 Binvox Voxel File Format Specification Author: Patrick Min Publication Date: N/A Last Modification Date: 2015-12-17 Published at: patrickmin.com Links: Website , Parent Page Last Viewed: 2020-06-03 SVX Format Specification Author: Alan Hudson (?) Publication Date: N/A Last Modification Date: N/A Published at: Absolutely Fabulous 3D Printing Links: Website Last Viewed: 2020-06-04 SLAB6 Project Page (KVX, KV6 Documentation) Author: Ken Silverman Publication Date: 2000 or earlier (?) Last Modification Date: 2011-03-22 Published at: Ken Silverman's Projects Page Links: Website , SLAB6 Download Last Viewed: 2020-06-07 Zoxel File Format Specification Author: Graham King Publication Date: 2013-05-16 Last Modification Date: 2013-11-29 Published at: Zoxel Repository Links: Wiki Page Last Viewed: 2020-06-08 Schematic File Format Authors: Minecraft Wiki Publication Date: N/A Last Modification Date: 2020-05-10 Published at: Minecraft Wiki Links: Wiki Page Last Viewed: 2020-06-08 Structure Block File Format Authors: Minecraft Wiki Publication Date: N/A Last Modification Date: 2020-05-25 Published at: Minecraft Wiki Links: Wiki Page Last Viewed: 2020-06-08 Sproxel User Manual Author: androo.gardner@gmail.com (?) Publication Date: N/A Last Modification Date N/A Published at: Sproxel at Google Code Links: Wiki Page Last Viewed: 2020-06-08 Web Compressing Voxel Worlds Author: Steven Pigeon Publication Date: 2011 or prior Published at: Harder, Better, Faster, Stronger Blog Links: HBFS Page Last Viewed: 2020-05-30","title":"References"},{"location":"related/literature.html#references","text":"","title":"References"},{"location":"related/literature.html#literature","text":"","title":"Literature"},{"location":"related/literature.html#specifications","text":"","title":"Specifications"},{"location":"related/literature.html#web","text":"","title":"Web"},{"location":"related/overview.html","text":"Overview over Related Work","title":"Overview over Related Work"},{"location":"related/overview.html#overview-over-related-work","text":"","title":"Overview over Related Work"},{"location":"related/voxel_formats.html","text":"List of Voxel File Formats Image-Based PNG Stack Stat Value Pattern *.png Media-Type image/png Magic \\x89PNG Software various Structure binary Volumes single Voxel-Encoding stack of images Compression PNG Color Support PNG Palette per-image, optional, PNG Minecraft Minecraft Schematic Stat Value Pattern *.schematic Media-Type application/x-schematic+nbt (unofficial) Magic \\x10 (Compound Tag) Software various Minecraft-related Structure binary, NBT subset Volumes single Voxel-Encoding voxel array Compression none Color Support MC block-based Palette implicit, global, MC blocks Minecraft Structure Stat Value Pattern *.mcstructure Media-Type application/x-minecraft-structure+nbt Magic \\x10 (Compound Tag) Software Minecraft Structure binary, NBT subset Volumes single Voxel-Encoding voxel list Compression none Color Support MC block-based Palette global, MC blocks Minecraft Region Stat Value Pattern *.mcr Media-Type application/x-minecraft-region Magic none Software Minecraft Structure binary, uses NBT Volumes 32x32 chunks, each 16x256x16 Voxel-Encoding voxel array Compression GZip oor Zlib Color Support MC block-based Palette global, MC blocks Qubicle Qubicle Exchange Format Stat Value Pattern *.qef Media-Type text/x-qef+plain (unofficial) Magic Qubicle Exchange Format ... (copyright header) Software Qubicle Structure text, line-based Volumes single Voxel-Encoding voxel list Compression none Color Support floating-point RGB Palette global Qubicle Binary Stat Value Pattern *.qb Media-Type model/x-qb (unofficial) Magic none Software Qubicle Structure binary Volumes multiple, named Voxel-Encoding voxel array Compression optional run-length encoding Color Support RGB24 Palette none Qubicle Binary Tree Stat Value Pattern *.qb Media-Type model/x-qb (unofficial) Magic QB 2 Software Qubicle Structure binary Volumes multiple, named, hierarchical Voxel-Encoding voxel array Compression zlib Color Support RGB24, RGBA32 with palette Palette optional, global Video Game Formats Ace of Spades Map File Format (aka. Voxlap) Stat Value Pattern *.vxl Media-Type model/x-vxl (unofficial) Magic none Software Ace of Spades Structure binary Volumes single Voxel-Encoding 512x512 map of voxel columns Compression none Color Support RGBA32 Palette none KVX Stat Value Pattern *.kvx Media-Type model/x-kvx (unofficial) Magic none Software Voxlap Structure binary Volumes single Voxel-Encoding map of voxel columns per mipmap level Compression none Color Support RGB666 Palette global, 256 colors SLABSPRI VOX Stat Value Pattern *.vox Media-Type model/x-slabspri-vox (unofficial) Magic none Software SLABSPRI.exe, SLAB6 Structure binary Volumes single Voxel-Encoding voxel array Compression none Color Support RGB666 Palette global, 256-1 colors Tiberian Sun Voxel Animation Format This format stores models for the video game Tiberian Sun by Westwood Studios. Stat Value Pattern *.3mp Media-Type model/x-tiberian-sun-vlx (unofficial) Magic Voxel Animation Software Tiberian Sun Structure binary Volumes multiple (limbs) Voxel-Encoding map of voxel columns per limb Compression none Color Support RGBA24 Palette 256 colors Voxel Editing Software Binvox Binvox is the format used by the popular voxelizer software binvox. The software voxelizes meshes but only preserves geometry which is why the file format has no color support. Both the software and the file format are still being used. Stat Value Pattern *.binvox Media-Type model/x-binvox (unofficial) Magic #binvox 1 (of which 1 is version) Software Binvox Structure binary with text header Volumes single1 Voxel-Encoding voxel array Compression run-length encoding Color Support none Palette none KV6 Stat Value Pattern *.kv6 Media-Type model/x-kv6 (unofficial) Magic Kvxl Software SLAB6 Structure binary Volumes single Voxel-Encoding map of voxel columns Compression none Color Support RGB666 in palette, RGB24 otherwise Palette optional, global, 256 colors Simple Voxels Simple Voxels is a file format introduced by Shapeways and largely oriented towards 3d printing. It takes the simple idea of an image stack and wraps this format within a zip-file, plus a manifest XML file. Stat Value Pattern *.svx Media-Type application/x-svx+zip (unofficial) Magic PK ... Software Shapeways Structure zip-archive with manifest.xml and images Volumes single Voxel-Encoding stack of images Compression depends on image format Color Support depends on image format Palette no global, only within image Sproxel CSV Stat Value Pattern *.csv Media-Type text/x-sproxel+csv (unofficial) Magic VOX Software Sproxel Structure CSV-based Volumes single Voxel-Encoding voxel array Compression none Color Support RGBA32 Palette none Sproxel Enhanced PNG This format uses custom PNG header fields to encode the 3d-dimensions of the model in the image. It stores slices of the model in the same image. Stat Value Pattern *.csv Media-Type image/x-sproxel+png (unofficial) Magic \\x89 PNG Software Sproxel Structure PNG-based Volumes single Voxel-Encoding stack of slices within single PNG Compression PNG Color Support PNG Palette PNG Magica Voxel Model The main format used by the extremely popular free voxel-editor MagicaVoxel. Due to its RIFF nature, it is comparably extendable and encodes various metadata in addition to the voxels itself. Stat Value Pattern *.vox Media-Type model/x-vox (unofficial) Magic VOX Software MagicaVoxel Structure binary with RIFF-style chunks Volumes multiple, since 0.99.5 with names, hierarchical, links Voxel-Encoding 8-bit voxel lists Compression none Color Support RGB24 Palette global, 256 colors of which one is reserved Zoxel Stat Value Pattern *.zox Media-Type text/x-zoxel+json (unofficial) Magic none Software Zoxel Structure text, JSON-based Volumes single Voxel-Encoding voxel list Compression none Color Support RGBA32 Palette none Model 3D M3M TODO write this section Paint3D Stat Value Pattern *.3mp Media-Type model/x-3mp (unofficial) Magic 3MP Software Paint3D Structure binary Volumes single Voxel-Encoding voxel array Compression optional RLE or zlib Color Support RGBA32 Palette none Experimental/Research 32-Bit Voxel List Stat Value Pattern *.vl32 Media-Type model/x-vl32 (unofficial) Magic none Software voxel-io Structure binary Volumes single Voxel-Encoding voxel list Compression none Color Support RGBA32 Palette none Proprietary VoxEdit Model This file format is proprietary and not much is known. Stat Value Pattern *.vxm Media-Type model/x-vxm (unofficial) Magic ??? Software VoxEdit Structure ??? Volumes single Voxel-Encoding ??? Compression ??? Color Support ??? Palette ??? BD Cubik Studio Project This is the file format used by the BD Cubik Studio abandondedware. It is proprietary and not much is known. Stat Value Pattern *.bdc|*.bdc3d Media-Type model/x-bdc (unofficial) Magic ??? Software BD Cubik Studio Structure binary Volumes multiple (free transform) Voxel-Encoding textured cuboid Compression ??? Color Support ??? Palette none Cubik Studio Project This is the file format used by the new version of Cubik Studio. It is proprietary and not much is known. Stat Value Pattern ??? Media-Type model/x-cubik-studio (unofficial) Magic ??? Software Cubik Studio Structure binary Volumes multiple Voxel-Encoding ??? Compression ??? Color Support ??? Palette ???","title":"List of Voxel File Formats"},{"location":"related/voxel_formats.html#list-of-voxel-file-formats","text":"","title":"List of Voxel File Formats"},{"location":"related/voxel_formats.html#image-based","text":"","title":"Image-Based"},{"location":"related/voxel_formats.html#minecraft","text":"","title":"Minecraft"},{"location":"related/voxel_formats.html#qubicle","text":"","title":"Qubicle"},{"location":"related/voxel_formats.html#video-game-formats","text":"","title":"Video Game Formats"},{"location":"related/voxel_formats.html#voxel-editing-software","text":"","title":"Voxel Editing Software"},{"location":"related/voxel_formats.html#experimentalresearch","text":"","title":"Experimental/Research"},{"location":"related/voxel_formats.html#32-bit-voxel-list","text":"Stat Value Pattern *.vl32 Media-Type model/x-vl32 (unofficial) Magic none Software voxel-io Structure binary Volumes single Voxel-Encoding voxel list Compression none Color Support RGBA32 Palette none","title":"32-Bit Voxel List"},{"location":"related/voxel_formats.html#proprietary","text":"","title":"Proprietary"},{"location":"rle/hilbert_curves.html","text":"3D Hilbert Curves Properties Hilbert curves are space-filling curves with numerous properties, beneficial for storage of multi-dimensional data. Let a Hilbert curve be a sequence h_n(i): \\mathbb{N} \\rightarrow \\mathbb{N}^3 where n \\in \\mathbb{N} is the iteration of the curve. the previous element h_n(i-1) is always a direct neighbor to any element of the curve h(i) the Manhattan distance of h_n(i-2) is always 2 the Manhattan distance of h_n(i-3) is either 1 or 3 it is possible to access elements h_n(i) randomly with O(n) complexity it is possible to access the index h^{-1}_n(p) of any point randomly with O(n) complexity Computing Hilbert Curves In this section, we will discuss how to compute elements of the Hilbert curve h(i) . In principle, the method to construct a Hilbert curve is as follows: Choose a base shape h_0(i), \\; 0 \\le i \\lt 8 that fills a 2x2x2 space (this is the first iteration) Find a table of rotations R so that a 2x2x2 block h_1(j) of h_0(i) resembles h_0(i) at a coarser level Use R to rotate shapes h_0 so they form one continuous curve The first two steps are just preparation. The last step needs to be performed recursively. Our goal is to rotate eight 2x2x2 pieces so that they form one continuous curve with the same coarse path as the shapes themselves. We then have a 4x4x4 shape with starts and ends in the same corners, allowing us to rotate it using exactly the same rules and connect it with seven more shapes to obtain an 8x8x8 shape etc. Due to this similarity between iterations, we only need a single base shape h_0 and rotation table R . Preparation As described, we must first find a fitting shape h_0 and a rotation table R . Figure 1: First Iteration of a 3D Hilbert Curve There are three shapes which can form a Hilbert curve, but this one in particular has properties that make it suited for high performance computing. In this case, we simply use a Gray Code of length 8, of which the three bits of each element are interpreted as coordinates. Figure 2: Second Iteration of a 3D Hilbert Curve Now, we come up with a rotation table R with 8 rotations R_i, \\;0 \\le i \\lt 8 for each of the 8 blocks h_1(i) . The blue voxel represents the first element of each block and the red element represents the last. Note how the global end and start voxels are in the same corners of the model in both the first and second iteration. Also note that the path taken through the blocks h_1(8i) is identical to the path taken within the shape h_0(i) . This is what it means that the blocks resemble the shape at a coarser level. Finding a rotation table R with such properties can be done using a brute-force algorithm. Algorithm Computing Hilbert curves is greatly simplified when first computing a Morton (Z-Order) curve and permuting its elements. A Morton curve also traverses space in 2x2x2 blocks recursively, but lacks many of the properties we discussed earlier. 3D Morton curves can be computed by simply interleaving the bits of any coordinate triple (x, y, z) , yielding a single number. With this intermediate step of morton-coding, our pseudo-code looks like this: vec3<unsigned> hilbert_to_xyz(unsigned hilbert, unsigned iteration) { unsigned morton = hilbert_to_morton(hilbert, iteration); vec3<unsigned> xyz = deinterleave3(morton); return xyz; } unsigned xyz_to_hilbert(vec3<unsigned> xyz, uint iteration) { unsigned morton = interleave3(xyz.x, xyz.y, xyz.z); unsigned hilbert = morton_to_hilbert3(morton, iteration); return hilbert; } As described earlier, we need to recursively apply our rotation table to blocks of 2x2x2 each. When our input is already morton-coded, each octal digit represents a position on one level of an octree. This means that we just need to rotate each digit correctly. A pseudo-code implementation of a hilbert_to_morton function looks as follows: unsigned hilbert_to_morton3(unsigned hilbert, unsigned iteration) { for (unsigned i = 0; i < iteration; ++i) { unsigned octal_digit = get_octal(hilbert, i); set_octal(hilbert, i, hilbert_to_morton_shape[octal_digit]); } unsigned first_digit = get_octal(hilbert, iteration); mat3 next_rot = rotation_table[first_digit]; for (unsigned i = iteration; i != 0; --i) { unsigned morton_digit = get_octal(hilbert, i - 1); unsigned rotated_digit = interleave3(next_rot * deinterleave3(morton_digit)); set_octal(hilbert, i -1, rotated_digit); next_rot = next_rot * hilbert_to_morton_rotations[morton_digit]; } return hilbert; } In the first loop, we simply perform our shape transform to each digit. This means that each hilbert-coded digit gets mapped onto a morton-coded digit. In the second loop, we apply the rotation of more significant digits to less significant digits. For non-linear transformations, we would have to use a nested loop and iterate over all the less-significant digits. However, in this case we only apply rotations and we can multiply the rotation matrices instead, exploiting associativity of matrix multiplication: M_1 (M_0 p) = (M_1 M_0) p Note that our rotation table needs to be indexed using morton-coded digits. unsigned hilbert_to_morton3(unsigned hilbert, unsigned iteration) { for (size_t shift = 0; shift < iteration * 3; shift += 3) unsigned hilbert_digit = (hilbert >> shift) & 0b111; unsigned morton_digit = hilbert_to_morton_shape[hilbertCodedDigt]; hilbert ^= (hilbert_digit ^ morton_digit) << shift; // replace digit in result } unsigned shift = iteration * 3; unsigned first_digit = (hilbert >> shift) & 0b111; mat3 next_rot = hilbert_to_morton_rotations[first_digit]; while (shift != 0) { shift -= 3; unsigned morton_digit = (result >> shift) & 0b111; unsigned rotated_digit = interleave3(next_rot * deinterleave3(morton_digit)); hilbert ^= (mortonDigit ^ rotated_digit) << shift; // replace digit in result next_rot = next_rot * hilbert_to_morton_rotations[morton_digit]; } return hilbert; }","title":"3D Hilbert Curves"},{"location":"rle/hilbert_curves.html#3d-hilbert-curves","text":"","title":"3D Hilbert Curves"},{"location":"rle/hilbert_curves.html#properties","text":"Hilbert curves are space-filling curves with numerous properties, beneficial for storage of multi-dimensional data. Let a Hilbert curve be a sequence h_n(i): \\mathbb{N} \\rightarrow \\mathbb{N}^3 where n \\in \\mathbb{N} is the iteration of the curve. the previous element h_n(i-1) is always a direct neighbor to any element of the curve h(i) the Manhattan distance of h_n(i-2) is always 2 the Manhattan distance of h_n(i-3) is either 1 or 3 it is possible to access elements h_n(i) randomly with O(n) complexity it is possible to access the index h^{-1}_n(p) of any point randomly with O(n) complexity","title":"Properties"},{"location":"rle/hilbert_curves.html#computing-hilbert-curves","text":"In this section, we will discuss how to compute elements of the Hilbert curve h(i) . In principle, the method to construct a Hilbert curve is as follows: Choose a base shape h_0(i), \\; 0 \\le i \\lt 8 that fills a 2x2x2 space (this is the first iteration) Find a table of rotations R so that a 2x2x2 block h_1(j) of h_0(i) resembles h_0(i) at a coarser level Use R to rotate shapes h_0 so they form one continuous curve The first two steps are just preparation. The last step needs to be performed recursively. Our goal is to rotate eight 2x2x2 pieces so that they form one continuous curve with the same coarse path as the shapes themselves. We then have a 4x4x4 shape with starts and ends in the same corners, allowing us to rotate it using exactly the same rules and connect it with seven more shapes to obtain an 8x8x8 shape etc. Due to this similarity between iterations, we only need a single base shape h_0 and rotation table R .","title":"Computing Hilbert Curves"},{"location":"rle/rle.html","text":"Run-Length Encoding Run-Length Encoding (RLE) is a form of lossless data compression which stores elements of said data using a single value and a count or \"run-length\". Viability for Voxel Compression RLE is viable in those cases, where there are long runs of identical or very similar data. Due to voxel arrays often containing huge amounts of empty space, they also contain long runs of 0x0 RGB values (or whatever other value represents empty voxels) which could be run-length-compressed. This mitigates their greatest downside compared to voxel lists , their empty-space-overhead. In-Band- vs Out-Of-Band- Signaling The main distinction in RLE methods can be made between In-Band Signaling And Out-Of-Band Signaling . In-Band Signaling This method uses the alphabet of the data which it attempts to compress. For instance, if the data consists of only alphabetic ASCII characters, then digits could be used to encode the counts. Or for ASCII characters which are stored in 8-bit integers, the uppermost, unused bit could be used to signal that the remaining seven bits are the count. In any case, this method is best when the addition of the count does not conflict with the existing data. Otherwise, an escape sequence is necessary. For instance, to encode any 8-bit integer sequence, the value 0xff could be used to \"escape\" the data and be followed up by the count and the actual byte to be encoded, including 0xff . \\text{aaabbbcfdeef} \\rightarrow \\text{3a3bcfd2ef} Out-Of-Band Signaling This method encodes all of the data in pairs of count and data instead of leaving single characters intact. Its main advantage is the simplicity of parsing and an identical effect for all types of data. While a particular escape sequence might be a poor choice for some specific data set, this method does not require an escape sequence and thus doesn't suffer from the problem. To avoid bloating the data in size, further measures must be taken such as having a sequence which can signal a sequence of different characters as well as a sequence of identical characters. For instance, negative counts could be used to signal multiple different characters. \\text{aaabbbcfdeef} \\rightarrow \\text{3a3b1c1f1d2e1f} Summary In-Band signaling should be used in cases where certain values in the data are unused, allowing for an escape sequence with no conflicts with other data. If there are not just a handful of values but an entire bit per byte which is unused (such as in ASCII ), then this method becomes especially viable. Otherwise, OOB signaling is preferable, as it gives much more flexibility to the implementing developer and doesn't produce any conflicts with the data to be compressed. Existing Implementations Binvox The Binvox file format provides simple compression for geometry-only voxel data. It uses out-of-band signaling After a text header, the binary voxel data can be specified as follows: struct binvox_data { marker[] } struct marker { u8 count u8 bit_value } Criticism Binvox run-length encodes only streaks of 0 -bits or 1 -bits. But to store their values, it uses an entire byte which wastes 7 bits. Almost half (7 out of 16) bits for each marker are thus wasted. Also the worst case is a recurring pattern of 010101... . For such a worst case, Binvox increases the required storage space by a factor of 16! Qubicle Binary The Qubicle Binary (QB) file format has an option for compressing models using RLE. It is still in use, although it has been superseded by the Qubicle Binary Tree (QBT) file format which uses zlib instead of custom RLE. Models are made of matrices , which are arrays, usually sized 128 3 or smaller 1 . When compressing matrices , each slice (xy-plane) is being run-length-encoded. The RLE implementation uses in-band signaling . The CODEFLAG = 2 escape sequence signals a following pair of count and data, whereas the NEXTSLICEFLAG = 6 signals the end of the current slice . Criticism This implementation is flawed, in that it fails to exploit an obvious non-conflicting escape sequence: A color with an alpha of zero is invisible, regardless of its other components. In the QB format, the escape sequences correspond to pitch black colors with an alpha of 2 or 6 respectively, which is almost invisible. Alpha channels can also be used to store visibility maps instead, where such values would be far more common. Such a problem could have been easily mitigated by using different values for the escape sequences or by not storing colors in RGBA or BGRA format. An example of a better constant would be 1 31 , which is interpreted as an invisible color but with a red value of 128. Our Experiments There are countless ways to implement RLE. We implemented a few experimental methods ourselves to see how much one could improve upon the original designs. Compact Binvox struct marker { bit value i7 count } Instead of wasting 7 bits, we simply integrate the bit-value into the uppermost bit of the count. If we read the marker as a signed two's complement integer, we can easily obtain the bit using marker < 0 and the count using marker & 0x7f . Example in (bitstream): 00001111 0 out (hex): 04 74 01 Complete Binvox / Bytewise Binvox struct marker { u8 count u8 value } Instead of wasting 7 bits, we allow for an entire byte to be repeated count times. Example in (bitstream): 00001111 0 out (hex): 0f 01 00 01 In-Band with 0x00 and 0xff Escape Sequences This method uses in-band signaling. We encode our bits as usual, but when we encounter an escape sequence, the following byte is used as our count. We have two escape sequences: 0x00 indicates that the following byte encodes the number of zero-bits 0xff indicates that the following byte encodes the number of one-bits The obvious advantage of this method is that our escape sequences don't conflict with high-entropy data. We preserve any bytes which have both 0 and 1 in them and only affect fully empty/filled bytes, which occur in a low-entropy context anyways. Example in (bitstream): 00001111 0 out (hex): 0f 00 in (bitstream): 00...00 (64 zeros total) out (hex): 00 40 Adaptive Contrary to in-band RLE, adaptive RLE addresses the issue of high-entropy sections by encoding those explicitly. In addition to markers for sequences of 0s and 1s, adaptive coding also allows for \"raw\" section markers where the following N bits are parsed as is. For this method, we use about one byte per run: struct marker { bit signal if signal == 0 { u8 raw_run_length bit[raw_run_length] raw_data } else { bit run_value u7 run_length } } Notice that due to the variable length of raw_data and due to the length of the tokens preceding it, this format is not aligned to byte boundary. This, in addition to the large complexity of encoding and decoding means that this is the slowest of all methods. Zlib TODO complete this For most of its lifetime, Qubicle did not support matrices greater than 128 in each dimension. However, the file format technically allows for greater sizes due to a 32-bit integer being used for each dimension. \u21a9","title":"Run-Length Encoding"},{"location":"rle/rle.html#run-length-encoding","text":"Run-Length Encoding (RLE) is a form of lossless data compression which stores elements of said data using a single value and a count or \"run-length\".","title":"Run-Length Encoding"},{"location":"rle/rle.html#viability-for-voxel-compression","text":"RLE is viable in those cases, where there are long runs of identical or very similar data. Due to voxel arrays often containing huge amounts of empty space, they also contain long runs of 0x0 RGB values (or whatever other value represents empty voxels) which could be run-length-compressed. This mitigates their greatest downside compared to voxel lists , their empty-space-overhead.","title":"Viability for Voxel Compression"},{"location":"rle/rle.html#in-band-vs-out-of-band-signaling","text":"The main distinction in RLE methods can be made between In-Band Signaling And Out-Of-Band Signaling .","title":"In-Band- vs Out-Of-Band- Signaling"},{"location":"rle/rle.html#existing-implementations","text":"","title":"Existing Implementations"},{"location":"rle/rle.html#our-experiments","text":"There are countless ways to implement RLE. We implemented a few experimental methods ourselves to see how much one could improve upon the original designs.","title":"Our Experiments"},{"location":"rle/space_filling_curves.html","text":"Space-Filling Curves A space-filling curve is a curve which's range contains the entirety of a hypercube. When filling space with a curve, we are generally applying a bijective function: c: \\mathbb{N} \\rightarrow \\mathbb{Z}^N,\\; N \\in \\mathbb{N} This maps discrete points of the curve onto points in (hyper)space. Note that often times it may be easier to iterate over all points in space and assign an index to each instead. Since our mapping is bijective, this is also a viable strategy for filling space, as long as we are working within finite boundaries. In such a case, we are applying the inverse function: c^{-1}: \\mathbb{Z}^N \\rightarrow \\mathbb{N},\\; N \\in \\mathbb{N} For the sake of simplicity, we will only be discussing three-dimensional space-filling curves c:\\mathbb{N} \\rightarrow \\mathbb{Z}^3 Motivation In the previous RLE methods, we have filled space using nested iteration. Using alternative space-filling curves improves spatial locality over nested iterations. This is expected to improve the compression ratio of RLE. Comparison of Space-Filling Curves We will be discussing three approaches, namely nested iteration , Z-Order Curves , and Hilbert Curves for use in RLE. First, we introduce, define and present an implementation of all three methods. Then, we compare them. One of the points of comparison is how frequently a jump is found in the curve. The set of jumps J(c) for a curve c is defined as: J(c) = \\{(i,j) \\in \\mathbb{N}\\times\\mathbb{N} \\;\\big|\\; \\lVert c(j) - c(i)\\rVert \\gt 1\\} The only curves without jumps are curves of which all points are direct neighbors. Note There are many more possible space-filling curves. Just by permuting coordinates we could obtain six variations of each of our curves. The three chosen examples were selected because they have significant advantages and disadvantages over one-another and thus act as a good proxy for comparison of all space-filling curves. Note Z-Order Curves are also known as Lebesgue curve , Morton Space-Filling Curve , Morton Order or Morton code . Nested Iteration Figure 1: Nested Iteration in Two Dimensions Let l_x, l_y, l_z \\in \\mathbb{N} be the dimensions of the volume to fill. Our curve can be defined as: c(i) = \\begin{pmatrix} i \\bmod (l_x \\cdot l_y) \\cr \\lfloor\\frac{i}{l_x}\\rfloor \\bmod l_y \\cr \\lfloor\\frac{i}{l_x \\cdot l_y}\\rfloor \\end{pmatrix} It is however much simpler to perform the inverse mapping: c^{-1}(x, y, z) = x + (y \\cdot l_x) + (z \\cdot l_x \\cdot l_y) For a cube with dimensions which are powers of two, this is equivalent to concatenating the bits of x , y and z of any point. Nested iteration fills space by iterating over all coordinates, with x running the fastest, followed by progressively slower-running coordinates. The reverse-approach can be performed without any multiplication at all, using the following code: voxel container[limit_x * limit_y * limit_z]; size_t index = 0; for (unsigned z = 0; z < limit_z; ++z) for (unsigned y = 0; y < limit_y; ++y) for (unsigned x = 0; x < limit_x; ++x, ++index) container[index] = voxel_at(x, y, z); Notice how our container can have any shape, it is not limited to a hypercube. A jump occurs once every limit_x coordinates, where x jumps from limit_x - 1 to 0 . Z-Order Curves Figure 2: Z-Order Curve in Two Dimensions Z-Order Curves have already been presented as a possible solution in 1996 by G.H. Morton . They have the significant advantage of a higher spatial locality. Z-Order Curves can be constructed iteratively by repeating the z-pattern each 2 N iterations, recursively. For three dimension, they can be defined as follows: \\begin{align} c(i) &= \\text{deinterleave}_3(i) \\\\ c^{-1}(x, y, z) &= \\text{interleave}_3(x, y, z) \\end{align} The operations \\text{interleave}_3 and \\text{deinterleave}_3 are the basis for Octree Node Indices . They (de)interleave the bits of the three coordinates, producing a single index which consists of a series of octal digits. See the linked section for an efficient implementation and thorough explanation. The space-filling itself can be implemented very similarly to nested iteration. We can implemented Z-Order Curves as follows: voxel container[limit * limit * limit]; for (unsigned z = 0; z < limit; ++z) for (unsigned y = 0; y < limit; ++y) for (unsigned x = 0; x < limit; ++x) container[interleave3(x, y, z)] = voxel_at(x, y, z); Note that our container now needs to be a cube and also must have dimensions which are a power of two. Interleaving bits is still the most expensive arithmetic operation performed here. A typical implementation will interleave each coordinate with zeros first, then combine these partial results using a bitwise OR. Only x changes every iteration, so we can cache the results for y and z . We can perform fewer operations in total using the following code: voxel container[limit_x * limit_y * limit_z]; size_t index = 0; for (unsigned z = 0; z < limit_z; ++z) { // assuming this is implemented as a special case which handles zero-inputs unsigned zi = interleave3(z, 0, 0); for (unsigned y = 0; y < limit_y; ++y) { unsigned yi = interleave3(0, y, 0); for (unsigned x = 0; x < limit_x; ++x, ++index) { unsigned xi = interleave3(0, 0, x); container[zi | yi | xi] = voxel_at(x, y, z); } } } Hilbert Curves Figure 3: Hilbert Curve in Two Dimensions Hilbert Curves also improve spatial locality similar to Z-Order curves. They are very similar to Z-Order Curves in the aspect that they also have a pattern for each 4 pixels (or 8 voxels) which is repeated recursively to construct the curve. However, Hilbert curves also require these units to be rotated depending on their position in the grid at a higher level. Despite that, we can construct Hilbert curves from Z-Order Curves . An efficient implementation of this was provided by Icabod . Figure 4: A Hilbert Curve in Three Dimension - Michael Trott, in The Mathematica GuideBook for Programming The implementation builds on our approach for Z-Order Curves. It can also be optimized by caching portions of our Morton (Z-Order) index, but the final conversion to our Hilbert index must happen in its entirety every iteration. voxel container[limit * limit * limit]; for (unsigned z = 0; z < limit; ++z) { for (unsigned y = 0; y < limit; ++y) { for (unsigned x = 0; x < limit; ++x) { unsigned morton_index = interleave3(x, y, z); unsigned hilbert_index = morton_to_hilbert3d(morton_index); container[hilbert_index] = voxel_at(x, y, z); } } } Comparisons Nested Iteration Z-Order Curve Hilbert Curve Complexity per Voxel O(1) O(\\log{b}) O(b) Container Restrictions cube with dims. 2 n cube with dims. 2 n Jumps each l_x iterations each 2 iterations never Implementation Effort low medium high Z-Order Curves have logarithmic complexity. However, hardware may support bit-(de)interleaving via a dedicated instruction which can be implemented very easily by rewiring bits. This can effectively turn the complexity to O(1) , should such hardware support exist. Also note that while Z-Order curves have very frequent jumps, the most frequents ones (each 2 iterations) are short, 2D-diagonal jumps covering a distance of \\sqrt{2} . Each 4 iterations, there is a longer 3D-diagonal jump of \\sqrt{3} . Each 8 iterations, there is an even longer jump, and so forth. Unlike the simple interleave3 operation for Z-Order curves, we can not easily hardware-optimize morton_to_hilbert3d . Each computed octal digit affects the transformation applied to less significant digits. This recursive dependency leads to a complexity of O(b) Impact on Run-Length Encoding The impact on RLE can be very positive or very negative depending on the used method. In the following table, we compare how our geometry compression methods are affected by the different curves: Method\\Size For Nested Iteration Z-Order Curve Hilbert Curve Binvox 177 KB 258 KB 184 KB Compact Binvox 89 KB 134 KB 98 KB Complete Binvox 254 KB 123 KB 109 KB In-Band 185 KB 93 KB 77 KB The surprising effect is that for our bitwise formats Binvox and Compact Binvox , the effect is negative. For byte-based formats like Complete Binvox and In-Band , the effect is positive. Negative Impact on Bitwise Formats The reason why bitwise formats are negatively impacted by higher spatial locality is that they rarely profit from its advantages but are very negatively impacted by its disadvantages. The main disadvantage which is a potentially higher transition rate is explained as follows: For bitwise, out-of-band RLE formats, it is possible to determine number of encoded markers using only the number of transitions t and the number of bits in the stream b (assuming, there is no size limit for the count stored in a marker). A transition occurs when a one-bit is followed by a zero-bit or a zero-bit is followed by a one-bit. When we run-length-encode a bitstream, we only need to encode a new marker for a transition. If all bits were equal, we would only need to encode a single marker storing the size of the bitstream and the value of said bit. Let t be the number of transitions and b the total number of bits in a stream. The transition rate is t_r = \\frac{t}{b} and the average run length r_\\oslash = t_r^{-1} = \\frac{b}{t} . We see the following results for our curves when iterating over our Perlin128 model: Stat\\Method Nested Iteration Z-Order Curve Hilbert Curve t (lower is better) 88936 125046 88410 t_r (lower is better) 0.042 0.0601 0.042 r_\\oslash (higher is better) 23.58 16.64 23.72 The transition rate is very negatively impacted by Z-Order Curves. This happens because the very frequent jumps which occur in Z-Order Curves will produce significant amounts of transitions from inside a shape (1) to outside a shape(0) at its boundaries. Note that Hilbert Curves have a lower transition rate but still negatively impact our bitwise formats. This only happens because with Hilbert Curves, we run out of space in our one-markers (8 bits for Binvox , 7 bits for Compact Binvox ) more frequently due to how efficient this reordering is. So ironically, better is worse in this case. Positive Impact on Bytewise Formats In the previous section, I mentioned that bitwise formats rarely profit from the advantages of spatial locality. Bytewise formats do profit and perform much better as a result. The primary effect of higher spatial locality is a \"clumping\" of transitions. While the number of transitions t may not be affected, sections of our model with more transitions will be placed closer together and sections with fewer transitions will also be placed together. This is reflected by another stat, the transition follow-up rate t_f , defined as follows: \\begin{align} \\text{is_transition}(i) &= (\\text{voxel_exists}(c(i)) \\ne \\text{voxel_exists}(c(i - 1))) \\\\ \\text{is_follow_up_transition}(i) &= \\text{is_transition}(i) \\land \\text{is_transition}(i-1) \\\\ t_f &= \\frac{\\lvert\\{0 \\le i \\lt b | \\text{is_follow_up_transition}(i)\\}\\rvert}{t} \\end{align} In other words: the probability that a transition is followed-up by another transition. Here the results for our Perlin128 model: Stat\\Method Nested Iteration Z-Order Curve Hilbert Curve t_f (higher is better) 0.01402 0.4346 0.2885 Bytewise formats do profit from a higher t_f , for a constant t . They can simply encode a byte with a run-length of one which stores multiple transitions. This leaves other areas which have very few transitions and can be encoded with longer runs. Bitwise formats can not do this and need a separate marker for every single transition. Note It is not required for a RLE format to encode 8-bit values to take advantage of a higher t_f . Any number of bits greater than one is sufficient. Summary We compared nested iteration , Z-Order Curves and Hilbert Curves for use in RLE. The impact of different space-filling curves was dramatic. Bytewise formats such as Complete Binvox and In-Band performed worse with just nested iteration but saw reductions in data of nearly 60% when using the latter methods. Surprisingly, the previously better-performing bitwise formats did not see any benefit. In fact, they became more redundant when using the latter space-filling curves. This phenomenon was investigated and explained by demonstrating that only the number of transitions affects the number of markers and thus the data size for bitwise formats. In particular, Z-Order Curves negatively impact the number of transitions. In total, we were able to achieve a reduction of 13% in data size using Hilbert Curves and In-Band over the previously best method, Compact Binvox with Nested Iteration . Unfortunately, the computational effort of Hilbert Curves puts into question whether a use of this method is desired.","title":"Space-Filling Curves"},{"location":"rle/space_filling_curves.html#space-filling-curves","text":"A space-filling curve is a curve which's range contains the entirety of a hypercube. When filling space with a curve, we are generally applying a bijective function: c: \\mathbb{N} \\rightarrow \\mathbb{Z}^N,\\; N \\in \\mathbb{N} This maps discrete points of the curve onto points in (hyper)space. Note that often times it may be easier to iterate over all points in space and assign an index to each instead. Since our mapping is bijective, this is also a viable strategy for filling space, as long as we are working within finite boundaries. In such a case, we are applying the inverse function: c^{-1}: \\mathbb{Z}^N \\rightarrow \\mathbb{N},\\; N \\in \\mathbb{N} For the sake of simplicity, we will only be discussing three-dimensional space-filling curves c:\\mathbb{N} \\rightarrow \\mathbb{Z}^3","title":"Space-Filling Curves"},{"location":"rle/space_filling_curves.html#motivation","text":"In the previous RLE methods, we have filled space using nested iteration. Using alternative space-filling curves improves spatial locality over nested iterations. This is expected to improve the compression ratio of RLE.","title":"Motivation"},{"location":"rle/space_filling_curves.html#comparison-of-space-filling-curves","text":"We will be discussing three approaches, namely nested iteration , Z-Order Curves , and Hilbert Curves for use in RLE. First, we introduce, define and present an implementation of all three methods. Then, we compare them. One of the points of comparison is how frequently a jump is found in the curve. The set of jumps J(c) for a curve c is defined as: J(c) = \\{(i,j) \\in \\mathbb{N}\\times\\mathbb{N} \\;\\big|\\; \\lVert c(j) - c(i)\\rVert \\gt 1\\} The only curves without jumps are curves of which all points are direct neighbors. Note There are many more possible space-filling curves. Just by permuting coordinates we could obtain six variations of each of our curves. The three chosen examples were selected because they have significant advantages and disadvantages over one-another and thus act as a good proxy for comparison of all space-filling curves. Note Z-Order Curves are also known as Lebesgue curve , Morton Space-Filling Curve , Morton Order or Morton code .","title":"Comparison of Space-Filling Curves"},{"location":"rle/space_filling_curves.html#impact-on-run-length-encoding","text":"The impact on RLE can be very positive or very negative depending on the used method. In the following table, we compare how our geometry compression methods are affected by the different curves: Method\\Size For Nested Iteration Z-Order Curve Hilbert Curve Binvox 177 KB 258 KB 184 KB Compact Binvox 89 KB 134 KB 98 KB Complete Binvox 254 KB 123 KB 109 KB In-Band 185 KB 93 KB 77 KB The surprising effect is that for our bitwise formats Binvox and Compact Binvox , the effect is negative. For byte-based formats like Complete Binvox and In-Band , the effect is positive.","title":"Impact on Run-Length Encoding"},{"location":"rle/space_filling_curves.html#summary","text":"We compared nested iteration , Z-Order Curves and Hilbert Curves for use in RLE. The impact of different space-filling curves was dramatic. Bytewise formats such as Complete Binvox and In-Band performed worse with just nested iteration but saw reductions in data of nearly 60% when using the latter methods. Surprisingly, the previously better-performing bitwise formats did not see any benefit. In fact, they became more redundant when using the latter space-filling curves. This phenomenon was investigated and explained by demonstrating that only the number of transitions affects the number of markers and thus the data size for bitwise formats. In particular, Z-Order Curves negatively impact the number of transitions. In total, we were able to achieve a reduction of 13% in data size using Hilbert Curves and In-Band over the previously best method, Compact Binvox with Nested Iteration . Unfortunately, the computational effort of Hilbert Curves puts into question whether a use of this method is desired.","title":"Summary"},{"location":"svo/construction.html","text":"SVO Construction When constructing an SVO from a voxel list , the complexity of this process depends on the data fed to the constructing algorithm. Within the scope of this project, the uncompressed format is a list of voxels , but there are further subtle differences. Summary of Construction The following process must be repeated for all voxels v := (p := (x, y, z), c) \\in (\\mathbb{Z}^3, \\mathbb{N}) in the list. Test signed input position p against current SVO dimensions d . If the position is outside of our boundaries, enlarge the SVO to fit p . Subtract p_{min} of the SVO from p to obtain p_{\\text{normalized}} . Convert p_{\\text{normalized}} to the octree node index n . Use n to traverse the SVO and insert the voxel at the correct location. Optional: If only one octant is used, cut branches belonging to other octants recursively. Note To handle signed values, the octree must be extended into both positive and negative octants. Coordinate System There are two coordinate systems with which we must concern ourselves with: world coordinate system (signed) SVO coordinate system (unsigned) The conversion between these two coordinate systems occurs in step 3. The problem which we face is that our SVO is meant to split the world coordinate system into eight equal octants, recursively. The first split occurs at the origin however, the origin (0, 0, 0) must be located in one of these octants. To solve this, we put the origin into the first octant; then all positions with no negative coordinates will lie in just one octant. This allows us to perform the optimisation from step 6 for only unsigned coordinates. It also helps us optimize our boundary test . As a result, for example, an SVO that can contain 4x4x4 voxels will have space from (-2, -2, -2) to (1, 1, 1) . Special Case Where Voxel Coordinates Are Positive If the voxels are sorted with the first voxel being the most negative corner of the model, we have a global p_{min} (see step 3.). All voxels can then be translated by -p_{min} , yielding only positions that belong in the first octant. If x, y, z are already guaranteed to be positive, this condition is obviously also met. In such cases. step 1. and 2. are simplified and step 6. is eliminated. Efficient Boundary Test To check whether a point lies within our SVO, max(abs(x), abs(y), abs(z)) > d would suffice. However, this test is overly pessimistic because we have more negative space instead of positive space. Also, assuming no compiler optimisations, we have to perform six conditional operations: each call to abs() requires one conditional operation a three-argument max() would require two more one more conditional operation is necessary for the > d comparison Assuming unsorted inputs, these six conditional jumps may wreak havoc on the performance of the branch predictor . To remove the pessimism from our test, we define a new function to be used in stead of abs : abs_{\\text{SVO}}(n) = \\begin{cases} -n - 1,& \\text{if } n < 0\\\\ n, & \\text{otherwise} \\end{cases} Implementation uint32_t abs_svo(int32_t n) { return n < 0 ? (-n -1) : n; } bool comp_against_svo_bounds(int32_t x, int32_t y, int32_t z, uint32_t d) { return (abs_svo(x) | abs_svo(y) | abs_svo(z)) >= d; } Such an implementation eliminates five out of six conditional operations on modern compilers, leaving only the final >= comparison. See this Compiler Explorer page for an example. Compiler Optimization of abs_svo Using abs_{\\text{SVO}} actually works in ours and the compiler's favor. This is due to the fact that negating a number in two's complement requires a bitwise negation and an increment. Due to our decrement of the negated number, we eliminate this need. abs_svo(int): mov eax, edi # copy function parameter into result register sar eax, 31 # fill the register with 1s if the number is negative, else with 0s xor eax, edi # xor result register with parameter, performing a conditional bitwise NOT retn # return from the function Manual Optimization of Comparison with d This only works because d is an SVO dimension and thus always guaranteed to be a power of two. The most significant bit of lower numbers is lower than the single bit of a greater power of two. To name an example, 128 will have the 128-bit set, whereas all lower numbers will consist only of less significant bits. Hence, they can be safely combined with an | operation before making the comparison. Summary The overly pessimistic initial comparison could be fixed with -surprisingly- no performance impact. Out of the six necessary conditional operations, we could optimize five away. During a microbenchmark of the original max(abs(x), abs(y), abs(z)) > d comparison vs. our optimized method, no performance difference could be found . However, keep in mind that abs() functions are often compiled to use a conditional move instruction which can be expensive on older architectures. So depending on the architecture, such a benefit could be seen. Octree Growth If we do find that a point which is to be inserted does not fit within the current octree, we must enlarge it. Single-Octant Growth Figure 1: Simple (Single-Octant) Octree Growth If only one octant is used, e.g. all positions are unsigned, then we can enlarge the octree into just one direction. The current root node goes into the lowest corner (with index 0) of the new, higher-level root node. Unilateral Octree Growth Figure 2: Unilateral Octree Growth If we use signed positions, we must grow our octree unilaterally. This means that each node receives a new parent. The four new parents are then moved into the root node at the location of their children. Here is an implementation in pseudo-C++: for (size_t i = 0; i < 8; ++i) { if (root.has(i)) { auto parent = make_new_branch(); parent[~i & 0b111] = root.extract(i); root[i] = parent; } } Within each new parent, the current nodes end up positioned in the opposite corner of where they were before. In one index from 0 to 7, each bit represents a three-dimensional coordinate. So index 4 = 0b010 represents (0, 1, 0) . By flipping all bits of the index we can quickly calculate the position inside the new parent. Note that for squashed octrees , this beautifully simple case no longer applies. We still create exactly eight new parents, but each parent will receive up to 4 of the 16 first-level branches. This and other problems make unilateral growth for squashed octrees a lengthy and complicated process. Position Normalization Once the octree has grown to a size at which it can contain our new position p = (x, y, z) , we must normalize our position. In this case normalization means that we simply subtract (x_{\\text{min}}, y_{\\text{min}}, z_{\\text{min}}) from p to obtain p_{\\text{normalized}} . This is necessary because internally, octrees don't have any concept of \"negative\" or \"positive\" positions, just indices within nodes which are all unsigned. The minimum coordinate for all dimensions is -2^d , where d is the unilateral depth of the octree. Once we subtract this minimum from our position, the position will be unsigned and ready for calculation of the octree node index. Octree Node Index After obtaining an unsigned position p_{\\text{normalized}} = (x, y, z) \\in \\mathbb{N}^3 within the octree, we must find out where to store this position in the data structure. Find the correct insertion point equires recursively testing whether each coordinate is in the lower or the upper half of current subtree. As long as the tree's dimensions are a power of 2, this can be reduced to checking whether a bit is set. For example, 128 is in the upper half of the 256-tree, because the most 128-bit is set, which is not the case for 127. We do not want to make this test at every level; It would be more convenient to pre-calculate a complete path through the octree that leads straight to the insertion point. Approach in One Dimension Figure: Binary Numbers Form a Binary Tree Implicitly As seen in the above Figure, the lower bit indicates whether the position is left or right in the lower subtree and the higher bit indicates whether the position is left or right in the upper subtree. To navigate the above binary tree, we can use the following pseudo-code: void insert(size_t index, rgb32_t color) { auto &branch = root[(index >> 1) & 1]; // use upper bit to get branch auto &leaf = branch[(index >> 0) & 1]; // use lower bit to get leaf leaf.color = color; } Approach in Three Dimensions The same pattern occurs for octal digits and octrees. (x, y, z) can thus be seen as three positions in separate binary trees which we want to combine into an octree. To convert (x, y, z) to a position in an octree, the bits of (x, y, z) can simply be interleaved. The result will be a single number of octal digits, each of which represents the position within one octree node. Just like we can use the bits of a binary number to navigate the above binary tree, we can use the digits of the octal number to navigate the octree at every level. void insert(size_t index, rgb32_t color) { auto &branch = root[(index >> 3) & 0b111]; // use upper digit to get branch // ... repeat this process for however many levels of branches there are auto &leaf = branch[(index >> 0) & 0b111]; // use lower digit to get leaf leaf.color = color; } Examples This is how coordinates can be mapped to octree indices: \\begin{align} & (6, 8, 9) = (\\color{red}{0101_2}, \\color{green}{1000_2}, \\color{blue}{1001_2}) \\\\ \\xrightarrow{\\text{interleave}} \\quad& (\\color{red}{0},\\color{green}{1},\\color{blue}{1}), (\\color{red}{1},\\color{green}{1},\\color{blue}{1}), (\\color{red}{0},\\color{green}{0},\\color{blue}{0}), (\\color{red}{1},\\color{green}{0},\\color{blue}{1}) \\\\ \\xrightarrow{\\text{concatenate}} \\quad& \\color{red}{0}\\color{green}{1}\\color{blue}{1}, \\color{red}{1}\\color{green}{1}\\color{blue}{1}, \\color{red}{0}\\color{green}{0}\\color{blue}{0}, \\color{red}{1}\\color{green}{0}\\color{blue}{1}_2 = 3705_8 = 1989 \\end{align} Note that in the above example, x is used as the most significant bit of each octal digit, followed by y and z . This is how octree node indices can be mapped to coordinates: \\begin{align} &25 = 31_8 = \\color{red}{0}\\color{green}{1}\\color{blue}{1},\\color{red}{0}\\color{green}{0}\\color{blue}{1}_2 \\\\ \\xrightarrow{\\text{to vectors}} \\quad& (\\color{red}{0}, \\color{green}{1}, \\color{blue}{1}), (\\color{red}{0}, \\color{green}{0}, \\color{blue}{1}) \\\\ \\xrightarrow{\\text{deinterleave}} \\quad& (\\color{red}{00_2}, \\color{green}{10_2}, \\color{blue}{11_2}) = (0, 2, 3) \\end{align} Implementation The following C++17 implementation shows how three coordinates (x, y, z) can be efficiently interleaved using binary Magic Numbers. The implementation expands upon one of the Bit Twiddling Hacks by Sean Eron Anderson . // interleaves a given number with two zero-bits after each input bit // the first insertion occurs between the least significant bit and the next higher bit uint64_t ileave_two0(uint32_t input) { constexpr size_t numInputs = 3; constexpr uint64_t masks[] = { 0x9249'2492'4924'9249, 0x30C3'0C30'C30C'30C3, 0xF00F'00F0'0F00'F00F, 0x00FF'0000'FF00'00FF, 0xFFFF'0000'0000'FFFF }; uint64_t n = input; for (int i = 4; i != 1; --i) { const auto shift = (numInputs - 1) * (1 << i); n |= n << shift; n &= masks[i]; } return n; } uint64_t ileave3(uint32_t x, uint32_t y, uint32_t z) { return (ileave_two0(x) << 2) | (ileave_two0(y) << 1) | ileave_two0(z); } Note An implementation for a variable amount of inputs is equally possible, but would significantly increase the code complexity. Most notably, the masks lookup table would need to be generated computationally. C++ was chosen due to its constexpr compile-time context. Traversing the Octree Once the octree node index is computed, traversing the octree becomes simple: For a given index n , we start at the most significant octal digit o and the root node. We follow the branch number o or construct it if it does not exist yet. Insert the voxel's color once a leaf node is found. Repeat until the least significant octal digit is processed. Note Once a nonexistent branch is found in step 2, all deeper branches will also be missing. In such a case, we can enter a different code path that handles this case. Node Implementation An SVO will need to store our voxel colors at some level. We can reduce memory consumption and cache misses by using a small voxel array at the second-to-final depth: // a regular node struct svo_node { svo_node *children[8]; }; // a node that stores colors instead struct svo_array_node { uint32_t colors[8]; } // a node that stores just one color, something that we want to avoid struct svo_leaf_node { uint32_t color; } This code will need to be adjusted so that the nodes are either polymorphic or type unions are used.","title":"SVO Construction"},{"location":"svo/construction.html#svo-construction","text":"When constructing an SVO from a voxel list , the complexity of this process depends on the data fed to the constructing algorithm. Within the scope of this project, the uncompressed format is a list of voxels , but there are further subtle differences.","title":"SVO Construction"},{"location":"svo/construction.html#efficient-boundary-test","text":"To check whether a point lies within our SVO, max(abs(x), abs(y), abs(z)) > d would suffice. However, this test is overly pessimistic because we have more negative space instead of positive space. Also, assuming no compiler optimisations, we have to perform six conditional operations: each call to abs() requires one conditional operation a three-argument max() would require two more one more conditional operation is necessary for the > d comparison Assuming unsorted inputs, these six conditional jumps may wreak havoc on the performance of the branch predictor . To remove the pessimism from our test, we define a new function to be used in stead of abs : abs_{\\text{SVO}}(n) = \\begin{cases} -n - 1,& \\text{if } n < 0\\\\ n, & \\text{otherwise} \\end{cases}","title":"Efficient Boundary Test"},{"location":"svo/construction.html#octree-growth","text":"If we do find that a point which is to be inserted does not fit within the current octree, we must enlarge it.","title":"Octree Growth"},{"location":"svo/construction.html#position-normalization","text":"Once the octree has grown to a size at which it can contain our new position p = (x, y, z) , we must normalize our position. In this case normalization means that we simply subtract (x_{\\text{min}}, y_{\\text{min}}, z_{\\text{min}}) from p to obtain p_{\\text{normalized}} . This is necessary because internally, octrees don't have any concept of \"negative\" or \"positive\" positions, just indices within nodes which are all unsigned. The minimum coordinate for all dimensions is -2^d , where d is the unilateral depth of the octree. Once we subtract this minimum from our position, the position will be unsigned and ready for calculation of the octree node index.","title":"Position Normalization"},{"location":"svo/construction.html#octree-node-index","text":"After obtaining an unsigned position p_{\\text{normalized}} = (x, y, z) \\in \\mathbb{N}^3 within the octree, we must find out where to store this position in the data structure. Find the correct insertion point equires recursively testing whether each coordinate is in the lower or the upper half of current subtree. As long as the tree's dimensions are a power of 2, this can be reduced to checking whether a bit is set. For example, 128 is in the upper half of the 256-tree, because the most 128-bit is set, which is not the case for 127. We do not want to make this test at every level; It would be more convenient to pre-calculate a complete path through the octree that leads straight to the insertion point.","title":"Octree Node Index"},{"location":"svo/construction.html#traversing-the-octree","text":"Once the octree node index is computed, traversing the octree becomes simple: For a given index n , we start at the most significant octal digit o and the root node. We follow the branch number o or construct it if it does not exist yet. Insert the voxel's color once a leaf node is found. Repeat until the least significant octal digit is processed. Note Once a nonexistent branch is found in step 2, all deeper branches will also be missing. In such a case, we can enter a different code path that handles this case.","title":"Traversing the Octree"},{"location":"svo/optimization.html","text":"SVO Optimization Sparse Voxel Octrees are often sufficiently compact data structures for computer graphics applications. However, they still contain a large amount of redundancy and have plenty of potential for optimization. We can differentiate between two types of optimizations: top-down optimizations optimize nodes starting from the root node and continue downwards. bottom-up optimizations optimize nodes starting from the leafs and continue upwards. Top-down optimizations alter the geometric boundaries of the octree but have little effect on the amount of nodes. This is due to the fact that in any tree, there are more nodes towards the bottom. Their primary purpose is accelerating access to the deeper layers. Bottom-up optimizations have no impact on the boundaries of the octree as this solely depends on the maximum depth. Their primary purpose is decreasing the amount of nodes or reducing nodes in size. Single-Octant Optimization Figure 1: Octree Optimization, where b is a sub-branch (visualized using a Quadtree) Once an octree has been fully constructed, unused octants can be optimized or \"cut away\" recursively. This can be especially helpful for octrees where all voxels reside very far from the origin. For instance, we could be encoding voxels with coordinates ranging from 100,000 to 100,050. Many almost completely empty octree layers would need to be traversed to get to where such locations are stored. Trimming away such single-octant layers accelerates encoding and decoding. This is our first and only top-down optimization . Algorithm s \\gets (0,0,0) If the root node r has exactly one branch b : s \\gets s + 2^{d-2} r \\gets b repeat 2. 2^d is the negated minimum point of our current octree, as described in Position Normalization . After this process has been completed, we simply store s alongside the octree. When decoding, s is added back onto all found positions. Complete Branch Optimization Figure 2: A Complete (Sub-)Tree Observe the Figure 2 above. All nodes are completely filled with 1 , representing that their subtree or color is present. This demonstrates one of the significant weak points of octrees: They are very good at trimming away air recursively, but have a very high cost when encoding a completely filled container. In fact, we would be much better off encoding such containers as arrays than using octrees, as octrees only add redundancy and no benefit in this case. To optimize such cases, we must eliminate nodes which are recursively filled in, or complete . A voxel model that demonstrates this well is chessmaster3 : Figure 3: chessmaster3 Model, visualized in Magica Voxel A very large portion of this model is completely filled in. In fact, the cuboid base of this model alone is 1088x1088 large and many voxels tall. When encoding this model, we see the following results: Method Geometry Bytes SVX 1031 KiB Octree 25044 KiB (~24x) Tetrahexacontree 22704 KiB (~22x) Octree Optimized 1340 KiB (~1.3x) Thct. Optimized 4619 KiB (~4.5x) So how could we accomplish a seemingly miraculous drop from 24x worse to nearly identical for octrees? By simply using the degenerate case of a zero-node to encode a complete subtree. Figure 4: The Tree from Figure 2, after optimization. Compare the above figure to Figure 2 . We are eliminating one layer of nodes and going from the root directly to the color information. Of course, the above data only includes geometry bytes, not color bytes. By eliminating such layers, we can save ourselves 8 nodes on the second level from the bottom, 64 nodes on the third, and so on. Geometrically, it does not even require a lot of space to be completely filled. We merely need a 4x4x4 volume of voxels to be complete in order to eliminate 8 nodes. Tetrahexacontrees do not perform nearly as well though. They require a 16x16x16 volume to be complete and only then is an optimization possible. The more squashed an octree becomes, the fewer opportunities there are to optimize it.","title":"SVO Optimization"},{"location":"svo/optimization.html#svo-optimization","text":"Sparse Voxel Octrees are often sufficiently compact data structures for computer graphics applications. However, they still contain a large amount of redundancy and have plenty of potential for optimization. We can differentiate between two types of optimizations: top-down optimizations optimize nodes starting from the root node and continue downwards. bottom-up optimizations optimize nodes starting from the leafs and continue upwards. Top-down optimizations alter the geometric boundaries of the octree but have little effect on the amount of nodes. This is due to the fact that in any tree, there are more nodes towards the bottom. Their primary purpose is accelerating access to the deeper layers. Bottom-up optimizations have no impact on the boundaries of the octree as this solely depends on the maximum depth. Their primary purpose is decreasing the amount of nodes or reducing nodes in size.","title":"SVO Optimization"},{"location":"svo/optimization.html#single-octant-optimization","text":"Figure 1: Octree Optimization, where b is a sub-branch (visualized using a Quadtree) Once an octree has been fully constructed, unused octants can be optimized or \"cut away\" recursively. This can be especially helpful for octrees where all voxels reside very far from the origin. For instance, we could be encoding voxels with coordinates ranging from 100,000 to 100,050. Many almost completely empty octree layers would need to be traversed to get to where such locations are stored. Trimming away such single-octant layers accelerates encoding and decoding. This is our first and only top-down optimization .","title":"Single-Octant Optimization"},{"location":"svo/optimization.html#complete-branch-optimization","text":"Figure 2: A Complete (Sub-)Tree Observe the Figure 2 above. All nodes are completely filled with 1 , representing that their subtree or color is present. This demonstrates one of the significant weak points of octrees: They are very good at trimming away air recursively, but have a very high cost when encoding a completely filled container. In fact, we would be much better off encoding such containers as arrays than using octrees, as octrees only add redundancy and no benefit in this case. To optimize such cases, we must eliminate nodes which are recursively filled in, or complete . A voxel model that demonstrates this well is chessmaster3 : Figure 3: chessmaster3 Model, visualized in Magica Voxel A very large portion of this model is completely filled in. In fact, the cuboid base of this model alone is 1088x1088 large and many voxels tall. When encoding this model, we see the following results: Method Geometry Bytes SVX 1031 KiB Octree 25044 KiB (~24x) Tetrahexacontree 22704 KiB (~22x) Octree Optimized 1340 KiB (~1.3x) Thct. Optimized 4619 KiB (~4.5x) So how could we accomplish a seemingly miraculous drop from 24x worse to nearly identical for octrees? By simply using the degenerate case of a zero-node to encode a complete subtree. Figure 4: The Tree from Figure 2, after optimization. Compare the above figure to Figure 2 . We are eliminating one layer of nodes and going from the root directly to the color information. Of course, the above data only includes geometry bytes, not color bytes. By eliminating such layers, we can save ourselves 8 nodes on the second level from the bottom, 64 nodes on the third, and so on. Geometrically, it does not even require a lot of space to be completely filled. We merely need a 4x4x4 volume of voxels to be complete in order to eliminate 8 nodes. Tetrahexacontrees do not perform nearly as well though. They require a 16x16x16 volume to be complete and only then is an optimization possible. The more squashed an octree becomes, the fewer opportunities there are to optimize it.","title":"Complete Branch Optimization"},{"location":"svo/svo.html","text":"Sparse Voxel Octree Figure 1: An Octree - Source: Wikipedia , WhiteTimberwolf A sparse voxel octree is a data structure which stores voxels in a tree with a branching factor of 8, with its branches being potentially absent. Missing branches typically represent empty volumes where no voxels exist. The greater empty volumes are, the closer to the root can their corresponding subtrees be pruned. This results in a very efficient representation of models with a significant portion of empty space. Unlike with a voxel list, all positioning is implicit and results from the tree structure, meaning that no space has to be used for the storage of coordinates during serialization. Thus, octrees combine the two greatest advantages of voxel lists and voxel arrays: like lists, they waste little space on encoding empty voxels like for arrays, voxel coordinates are implicit and little space is wasted Extreme Cases And Limits To illustrate the following extreme cases, voxel arrays and voxel lists are also compared. The space complexity of three extreme cases is compared between the three data structures, where v is the amount of voxels. Voxel Array Voxel List Sparse Voxel Octree Empty O(1) 1 O(1) O(1) Tightly Filled O(v) O(4v) O(\\frac{8}{7} v) Stretched O(\\infty) O(1) O(\\infty) Empty Octree Entirely empty models can be encoded using just the root note, which then encodes that no subtrees exist. Tightly Filled Octree Entirely filled models will have some overhead compared to an array of voxels. For every eight nodes, there is one parent node. We can calculate the maximum possible overhead by summing up this \\frac{1}{8} overhead infinitely: \\sum_{n=1}^\\infty{\\frac{1}{8}^n} = \\frac{1}{7} So at worst, our data will increase by \\frac{1}{7} , which is much better than a 100% increase such as for binary trees. Stretched Octree The stretched case is a case where a finite amount of voxels are placed infinitely far apart. For arrays, this produces infinite space requirements because all space between these points must be filled. For octrees, more layers are necessary to encode positions further from the origin. In neither case there is an upper bound to this. In practice, octrees perform significantly better at encoding sparse data than arrays. Construction and Optimization How an octree can be constructed from a list voxels is thorougly explained in SVO Construction . After the octree has been constructed, it will often be necessary to optimize its structure. See SVO Optimization . Serialization Figure 2: A Sparse Voxel Octree, encoded in memory To be used in a serial data format, octrees must first be serialized. Nodes will no longer be laid out freely in memory but instead be arranged one after another. To fully encode an SVO, two steps must be performed: Linearize nodes by traversing the SVO's nodes in a deterministic, reversible order. Serialize each node to binary data. Traversal Order There are two well-known strategies for traversing trees completely: Depth-First Search (DFS) Breadth-First Search (BFS) We improve upon Depth-First Search using our own novel method, Accelerated Depth-First Search (ADFS). The main points of comparison are as follows: Method Space Data Structure Bulk-Read Possible DFS O(\\log{v}) Stack of Nodes never BFS O(v) Queue for entire layer ADFS O(\\log{v}) Stack of Node Lists for direct children Depth-First Figure 3: Tree, traversed depth-first DFS can be performed using only a stack to keep track of the node number at each level. On the deepest level, the next node is chosen until the end is reached and the next parent node is chosen. This low memory cost (which is in fact O(\\log{n}) where n is the number of nodes) is highly advantageous when encoding enormous models. Breadth-First Figure 4: Tree, traversed breadth-first BFS comes with a higher cost since a typical algorithm appends all branches to a queue for every traversed node. This means that in the worst case, which is at the beginning of serialization eight nodes are appended on every level before any node is popped from the queue, resulting in a higher memory cost. Accelerated Depth-First Figure 5: Tree, traversed depth-first In Figure 3 , the labels symbolize order of storage / order of visitation . Accelerated Depth-First is similar to DFS, however all direct children of the current node are stored first. So for example, we store all direct children (2, 3, 4) of the root node (1), but then continue onward to 2 (without storing 2 again) as though we were performing regular DFS. Implementation The implementation is trivial if we are already capable of performing DFS. Instead of storing the node which we visit during DFS, we store all of its children. When we visit the next node, it has always already been written because it is the child of some previous node. Of course this is not true for the root node, which we need to store first. Advantages When we are decoding a format in which a node stores the connections to its child nodes, we can count the number of connections and read multiple nodes simultaneously. This is the main benefit of this method. Regular DFS requires us to inspect one connection at a time, then go one node deeper into the tree if it is set. A bulk-read is never possible. An obvious prerequisite is that the nodes are encoded in a fixed-length format, otherwise we can't safely perform such a bulk-read. We can still use just a stack as a data structure, but we need to store a list at every depth with the cached nodes. Note This approach was specially invented for this research project. Any previous use of it is unknown to me. Why To Serialize Octrees Depth-First and not Breadth-First Figure 6: A serialized octree, depth-first The scheme more practical for encoding octrees is depth-first. This is due to the fact that only a stack is necessary to keep track of the current position. The size or depth of the octree would need to double in all dimensions to necessitate a stack greater by one element. Overall, this is a very low memory profile. For breadth first, we would always load information about the entire next deeper layer into our queue, then move on. To be fair, this does allow us to discard the previous layer entirely once we move deeper, but the memory cost is still unnecessarily high. Node Encoding In the above graphs, the 1-bit encoding was demonstrated due to its simplicity. However, there is a variety of different encodings to choose from. Single-Bit Format 0 stands for air-subtree 1 stands for partially or entirely filled voxel-subtree 8 Bits per octree-node, thus this format is byte-aligned. 2-Bit Format 00 stands for air-subtree 01 stands for solid subtree 10 stands for partially filled subtree 11 variable purpose 1.5-Bit Format 00 stands for air-subtree 01 stands for solid subtree 1 stands for partially filled subtree Variable amount of bits per octree-node, neither bit- nor byte-aligned. On the last level (node = voxel), the single-bit format is used because there are no subtrees. 11 for Raw/Array Subtrees Useful to encode subtrees completely filled with high-entropy content. On the second-to-last level, 11 for raw-subtrees there is no difference between 10 and 11 , so reverting to the 1.5-bit format is viable. On the last level, the single-bit format should be used. As long as the 1.5-bit format is not used, this encoding is completely byte-aligned: * alignment to 16 -bit boundaries on most levels * alignment to 8 -bit boundaries on last level 11 for Pointers to Subtrees on the Same Level Useful for encoding multiple similar subtrees. Similarities between structures could be effectively exploited. Verifying whether subtrees are equal down to the base level has very high complexity: O(n^2 * 8^n) . The comparison depth could be reduced down to a limited number of levels or the level number could be encoded next to the pointer: {u32 indexOfOtherTree, u8 depth} . Instead of the pointer, a tree could instead encode all the places where it is additionally used. This would make decoding easier, since remembering trees on the same level is not necessary. Degenerate Cases If you paid close attention to Figure 1 and Figure 3 , you will have noticed that they contain a 00000000 node. Such nodes can in principle exist, but are a waste of space. After all, they could be trimmed away one layer above by simply encoding a 0 -bit. This nonsensical nature is why they are considered to be a degenerate case . Such cases can however be useful. They offer a natural escape sequence from the regular encoding of an SVO. They could offer special functionality as seen in 3.2.3 to the single-bit format, which does not have any unused values. Squashed Octrees Figure 7: A squashed Octree (compare to Figure 0) A squashed octree is an octree where two or more layers of the tree have been combined into a single layer. By squashing an octree once, we receive a tetrahexacontree. This term comes from \"tetrahexaconta\" (Greek for 64) and \"tree\". It builds on the idea of octrees but expands the branching factor from 8 to 8 2 , or 64. In almost all regards, a squashed octree is implemented identically to a regular octree. However, we divide space into a 4x4x4 cube instead of a 2x2x2 cube with each layer. Motivation When serializing or deserializing an octree, we must do so node by node. This involves generating a bitmask (see above sections on node encoding). For a tetrahexacontree and under use of the 1-bit format, we fill exactly one CPU register on a 64-bit architecture with our nodes. With a regular octree, we would waste all but eight bits. 64-bit architectures established themselves as the de-facto standard for desktop operating systems. Anecdotally, \"macOS Mojave would be the last version of macOS to run 32-bit apps\" - Apple . Thus, optimization of algorithms in the present day can be performed for 64-bit architectures without worrying about 32-bit users. A use of this concept can be seen in the VOLA file format. VOLA uses a 1-bit format for its nodes while squashing two layers into one. So VOLA uses a tetrahexacontree. Unfortunately the paper fails to mention how performance and compression efficiency can be gained or lost this way. Therefore, we run our own tests in the following subsections. Performance Benefits Squashed SVOs lead to very significant performance benefits. For instance, when storing our Ragged Cluster model voxel-by-voxel in our SVO, this took: for a regular SVO: 1700-1800 ms for a squashed SVO: 1048-1288 ms It would be a realistic estimate to say that a 100% speedup is possible. Spatial Costs The costs of one squash are almost neglegible. For instance, our Ragged Cluster model requires: for a regular SVO: 50,467,464 node bits in single-bit format for a squashed SVO: 50,566,592 node bits in single-bit format That is a mere 0.196% increase in bits. However, while by a slim margin, a tetrahexacontree will almost always perform slightly worse: Best Case for Regular Octrees In the best case which is an entirely empty node, an octree requires only one byte of space to encode that each of the 8 child-nodes are empty. A tetrahexacontree will however require eight times the space, meaning 64 bits that are all 0 . However, octrees are very good at trimming away large empty volumes. For our Ragged Cluser , the average sub-branch count was 6.901 , meaning that in each node we would most likely see 7 out of 8 bits set. So this best case is rarely encountered. Best Case for Squashed Octrees The only case where a squashed octree is more efficient than a regular octree is the case where all sub-branches exist. This would be one root-node and 8 branches for a regular octree. Thanks to the squash, this this would only be a single node for a tetrahexacontree, but eight times as large. For our Ragged Cluser , the average squashed sub-branch count was 35.78 . So we are far from encountering this completely-filled case. Squashing More than One Layer In principle we squash even more layers, to receive a tree with a branching factor of 512 and higher. However, we would not receive architectural benefits anymore and would lose out on even more compression efficiency. Conclusion Tetrahexacontrees provide nearly identical spatial costs while providing a very significant performance improvement. This result does have broad implications for the use of octrees in computer graphics in general. Many ray casting algorithms would likely benefits from higher branching factors. In our case, we will follow the example of the VOLA file format and use a higher branching factor in our encoding scheme. Depends on how much space is pre-allocated before the insertion of any voxels. Other data structures consume no space for such a pre-allocation. \u21a9","title":"Sparse Voxel Octree"},{"location":"svo/svo.html#sparse-voxel-octree","text":"Figure 1: An Octree - Source: Wikipedia , WhiteTimberwolf A sparse voxel octree is a data structure which stores voxels in a tree with a branching factor of 8, with its branches being potentially absent. Missing branches typically represent empty volumes where no voxels exist. The greater empty volumes are, the closer to the root can their corresponding subtrees be pruned. This results in a very efficient representation of models with a significant portion of empty space. Unlike with a voxel list, all positioning is implicit and results from the tree structure, meaning that no space has to be used for the storage of coordinates during serialization. Thus, octrees combine the two greatest advantages of voxel lists and voxel arrays: like lists, they waste little space on encoding empty voxels like for arrays, voxel coordinates are implicit and little space is wasted","title":"Sparse Voxel Octree"},{"location":"svo/svo.html#extreme-cases-and-limits","text":"To illustrate the following extreme cases, voxel arrays and voxel lists are also compared. The space complexity of three extreme cases is compared between the three data structures, where v is the amount of voxels. Voxel Array Voxel List Sparse Voxel Octree Empty O(1) 1 O(1) O(1) Tightly Filled O(v) O(4v) O(\\frac{8}{7} v) Stretched O(\\infty) O(1) O(\\infty)","title":"Extreme Cases And Limits"},{"location":"svo/svo.html#construction-and-optimization","text":"How an octree can be constructed from a list voxels is thorougly explained in SVO Construction . After the octree has been constructed, it will often be necessary to optimize its structure. See SVO Optimization .","title":"Construction and Optimization"},{"location":"svo/svo.html#serialization","text":"Figure 2: A Sparse Voxel Octree, encoded in memory To be used in a serial data format, octrees must first be serialized. Nodes will no longer be laid out freely in memory but instead be arranged one after another. To fully encode an SVO, two steps must be performed: Linearize nodes by traversing the SVO's nodes in a deterministic, reversible order. Serialize each node to binary data.","title":"Serialization"},{"location":"svo/svo.html#squashed-octrees","text":"Figure 7: A squashed Octree (compare to Figure 0) A squashed octree is an octree where two or more layers of the tree have been combined into a single layer. By squashing an octree once, we receive a tetrahexacontree. This term comes from \"tetrahexaconta\" (Greek for 64) and \"tree\". It builds on the idea of octrees but expands the branching factor from 8 to 8 2 , or 64. In almost all regards, a squashed octree is implemented identically to a regular octree. However, we divide space into a 4x4x4 cube instead of a 2x2x2 cube with each layer.","title":"Squashed Octrees"}]}